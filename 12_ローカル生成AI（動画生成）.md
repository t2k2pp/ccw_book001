# 第12章: ローカル生成AI（動画生成）

## 12.1 ローカル動画生成AIの概要とメリット

### ローカル実行の利点

**プライバシーとセキュリティ**
- 動画コンテンツがローカル環境に留まり、外部サーバーに送信されない
- 企業の機密映像や個人的なビデオ素材の安全な処理
- クラウドサービスで制限される内容の自由な生成

**コストメリット**
- 初期投資後は無制限に動画生成が可能
- クラウドサービス（Runway Gen-3: $12/月〜、Pika: $10/月〜）と比較して長期的にコスト削減
- 大量の動画生成が必要な場合に特に有利

**カスタマイズ性とコントロール**
- フレーム単位での細かい制御が可能
- カスタムモーションモジュールの学習
- ControlNetやIPAdapterによる詳細な構図・スタイル制御

**オフライン動作**
- インターネット接続不要で安定動作
- レンダリング時間が長くても途中で中断されない

### 主な用途

- ショート動画制作（YouTube Shorts、TikTok、Instagram Reels）
- アニメーション・モーショングラフィックス
- プロトタイピング・コンセプト映像
- ゲーム開発用カットシーン
- マーケティング動画素材
- 教育コンテンツのビジュアライゼーション
- ミュージックビデオ制作

### 現状の課題

- **VRAM要件が高い**: 最新モデルは12GB以上、高品質では24GB以上推奨
- **生成時間が長い**: 数秒の動画でも数分〜数十分かかる
- **動画の長さ制限**: ほとんどのモデルは5-10秒程度が上限
- **品質の一貫性**: フレーム間の連続性やモーション品質が課題

---

## 12.2 マシンスペック別推奨モデル

### 最小構成（VRAM 8-10GB）

**推奨GPU**: RTX 3060 12GB、RTX 4060 Ti 16GB

**対応モデル**:
- AnimateDiff（テキスト→動画、低解像度）
- Stable Video Diffusion（画像→動画、512×512）

**制限事項**:
- 解像度は512×512または768×768が限界
- フレーム数は16-24フレーム程度（0.5-1秒）
- バッチ処理は不可

**推奨用途**:
- 短いGIFアニメーション
- プロトタイピング
- 学習・実験用

---

### 標準構成（VRAM 12-16GB）

**推奨GPU**: RTX 4060 Ti 16GB、RTX 4070 12GB、RTX 3080 12GB

**対応モデル**:
- CogVideoX 2B（テキスト→動画、画像→動画）
- AnimateDiff（高解像度、複数ControlNet）
- LTX-Video（高速生成）
- Mochi 1（量子化版）

**推奨用途**:
- 短編動画制作（3-5秒）
- ソーシャルメディア用コンテンツ
- キャラクターアニメーション
- video-to-video変換

---

### 推奨構成（VRAM 24GB）

**推奨GPU**: RTX 4090 (24GB)、RTX A5000 (24GB)

**対応モデル**:
- CogVideoX 5B（FP16完全版、最大6秒動画）
- Mochi 1（10B、480p、5.4秒）
- Hunyuan Video（量子化版）
- AnimateDiff（最高品質、複数エフェクト）

**推奨用途**:
- プロフェッショナルな短編動画
- 高品質なプロモーション映像
- 複雑なモーション・エフェクト
- 商用コンテンツ制作

---

### ハイエンド構成（VRAM 40GB+）

**推奨GPU**: RTX A6000 (48GB)、A100 (40-80GB)、H100 (80GB)

**対応モデル**:
- Hunyuan Video（13B、FP16完全版）
- 複数モデルの同時実行
- 長尺動画生成（10秒以上）
- カスタムファインチューニング

**推奨用途**:
- 映画・CMクオリティの映像制作
- モデル開発・研究
- 大規模商用プロジェクト

---

### 🆕 コンパクト・統合GPU構成（AMD Strix Halo）

**MINISFORUM MS-S1 MAX** ¥345,000 (約$2,299)

**スペック**:
- APU: AMD Ryzen AI Max+ 395
- 統合GPU: Radeon 8060S（最大96GB VRAM利用可）
- RAM: 128GB LPDDR5X-8000
- TDP: 110-160W
- NPU: 50 TOPS（動画前処理に活用可能）

**対応モデル**:
- Mochi 1（10B、480p）
- CogVideoX 5B（FP16完全版）
- Hunyuan Video（量子化版、FP8）
- AnimateDiff（複数バリアント同時実行）

**推奨用途**:
- 省スペースで大型動画モデル実行
- 複数モデルの同時実行（Mochi + CogVideoX）
- バッチ処理（複数動画の並列生成）
- コストを抑えた商用動画制作

**利点**:
- 96GB VRAM（Hunyuan Video量子化版も快適）
- 省スペース・ミニPC（デスクトップ不要）
- 電力効率が良い（RTX 4090比で1/2以下）
- クラスタ構成も容易（2台で超大型モデル対応）

**注意点**:
- 生成速度はRTX 4090比で30-40%低速
- メモリ帯域（256GB/s）がボトルネックになる場合あり
- 単一高速生成よりバッチ処理向き

**実測パフォーマンス**:
- Mochi 1 (10B): 約5-7分/動画（5秒）
- CogVideoX 5B: 約3-5分/動画（6秒）
- 2台クラスタ: 大型モデルで実用的な速度

---

## 12.3 動画生成モデルカタログ（2025年版）

### ⭐ Hunyuan Video（最推奨 - 最高品質）

**開発**: Tencent（騰訊）
**リリース**: 2024年12月
**パラメータ**: 13B

**主な特徴**:
- **最高レベルの品質**: 映画的なクオリティ、強力な物理精度とシーン一貫性
- **複雑なモーション**: 連続的で複雑な動きに特化
- **ベンチマーク**: Luma 1.6など複数のクローズドソースモデルを上回る（テキスト整合性、モーション品質、ビジュアル品質で優位）

**スペック**:

| 項目 | 仕様 |
|------|------|
| 解像度 | 最大720p |
| フレーム数 | 最大5秒（約150フレーム@30fps） |
| VRAM要件 | 最小60GB（最適化版で24GB程度まで削減可能） |
| 推論速度 | 遅い（H100で5-10分/動画） |

**推奨用途**:
- プロフェッショナルな映像制作
- 商用CMや広告映像
- 高品質なストーリーテリング

**課題**:
- 非常に高いVRAM要件（量子化・最適化が進行中）
- 生成時間が長い

**セットアップ例**:
```bash
git clone https://github.com/Tencent-Hunyuan/HunyuanVideo.git
cd HunyuanVideo

# モデルのダウンロード（Hugging Face経由）
huggingface-cli download Tencent/HunyuanVideo --local-dir models/

# 推論実行（要60GB+ VRAM）
python inference.py --prompt "A cinematic shot of a sunset over mountains" --output output.mp4
```

---

### ⭐ Mochi 1（推奨 - バランス型）

**開発**: Genmo
**リリース**: 2024年10月
**パラメータ**: 10B

**主な特徴**:
- **オープンソースで最高のテキスト→動画品質**
- **プロンプト精度**: 印象的な一貫性で詳細を正確に再現（特定のカメラアングル、照明条件など）
- **自然な動き**: モーション品質に優れる
- **Apache 2.0ライセンス**: 商用利用可能

**スペック**:

| 項目 | 仕様 |
|------|------|
| 解像度 | 480p（720p対応予定） |
| フレーム数 | 最大162フレーム（5.4秒@30fps） |
| VRAM要件 | 12-24GB（量子化で12GB動作可能） |
| 推論速度 | 中速（RTX 4090で3-5分/動画） |

**推奨用途**:
- 高品質なテキスト→動画生成
- プロンプトに忠実な映像が必要な場合
- 商用プロジェクト（ライセンスがクリア）

**セットアップ例**:
```bash
# ComfyUIでMochi 1を使用
cd ComfyUI/custom_nodes
git clone https://github.com/kijai/ComfyUI-MochiWrapper.git
cd ComfyUI-MochiWrapper
pip install -r requirements.txt

# モデルのダウンロード
cd ../../models
wget https://huggingface.co/genmo/mochi-1-preview/resolve/main/mochi_preview_dit_fp8_e4m3fn.safetensors
```

---

### ⭐ CogVideoX 5B（推奨 - 実用性重視）

**開発**: Tsinghua University & ZhipuAI
**リリース**: 2024年8月（5B）、2025年1月（2B）
**パラメータ**: 2B / 5B

**主な特徴**:
- **実用的なVRAM要件**: 5Bモデルでも12-16GBで動作（最適化済み）
- **テキスト→動画 & 画像→動画**: 両方に対応
- **比較的高速**: RTX 4090で数分で生成可能
- **良好な品質**: 価格対性能比が優秀

**スペック**:

| モデル | パラメータ | VRAM要件 | 解像度 | フレーム数 |
|--------|------------|----------|--------|------------|
| CogVideoX 2B | 2B | 8-12GB | 720×480 | 最大6秒 |
| CogVideoX 5B | 5B | 12-16GB | 720×480 | 最大6秒 |

**推奨用途**:
- 中品質の動画生成（SNS用途など）
- 画像→動画変換（img2vid）
- 標準的なVRAMで動作させたい場合

**セットアップ例**:
```bash
# ComfyUIでCogVideoXを使用
cd ComfyUI/custom_nodes
git clone https://github.com/kijai/ComfyUI-CogVideoXWrapper.git
pip install -r ComfyUI-CogVideoXWrapper/requirements.txt

# モデルのダウンロード
cd ../models/CogVideo
wget https://huggingface.co/THUDM/CogVideoX-5b/resolve/main/transformers/diffusion_pytorch_model.safetensors
```

---

### AnimateDiff（軽量・高速）

**開発**: GuoYuWei et al.
**リリース**: 2023年7月（v1）、2024年3月（v3）
**ベース**: Stable Diffusion 1.5

**主な特徴**:
- **低VRAM**: 8-10GBで動作
- **豊富なモーションモジュール**: コミュニティによる多様なモジュール
- **ComfyUI統合**: 強力なワークフローエコシステム
- **video-to-video変換**: 既存動画のスタイル変換に強い

**スペック**:

| 用途 | VRAM要件 | 解像度 | フレーム数 |
|------|----------|--------|------------|
| テキスト→動画 | 8GB | 512×512 | 16-24フレーム |
| 動画→動画 | 10GB | 512×512 | 24-48フレーム |
| 高解像度 | 12GB+ | 768×768 | 16-24フレーム |

**推奨用途**:
- 低スペックPCでの動画生成
- GIFアニメーション
- 既存動画のスタイル変換（アニメ化など）
- ControlNetを使った構図制御

**セットアップ例**:
```bash
# ComfyUI AnimateDiff Evolved（推奨）
cd ComfyUI/custom_nodes
git clone https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git
cd ComfyUI-AnimateDiff-Evolved
pip install -r requirements.txt

# モーションモジュールのダウンロード
cd ../../custom_nodes/ComfyUI-AnimateDiff-Evolved/models
wget https://huggingface.co/guoyww/animatediff/resolve/main/mm_sd_v15_v2.ckpt
```

---

### Stable Video Diffusion（画像→動画特化）

**開発**: Stability AI
**リリース**: 2023年11月

**主な特徴**:
- **画像→動画に特化**: 1枚の画像から短い動画を生成
- **比較的低VRAM**: 12GB程度で動作
- **安定した品質**: Stability AIの信頼性

**スペック**:

| 項目 | 仕様 |
|------|------|
| 入力 | 1024×576画像 |
| 出力 | 25フレーム（約1秒@25fps） |
| VRAM要件 | 12GB以上推奨 |

**推奨用途**:
- 静止画のアニメーション化
- カメラモーション追加（ズーム、パンなど）

---

### LTX-Video（超高速）

**開発**: Lightricks
**リリース**: 2024年12月

**主な特徴**:
- **非常に高速**: リアルタイムに近い生成速度
- **低VRAM**: 8-12GBで動作
- **トランスフォーマーベース**: 効率的なアーキテクチャ

**推奨用途**:
- プロトタイピング
- リアルタイムプレビュー
- 大量の動画バリエーション生成

---

## 12.4 用途別モデル選択ガイド

### 最高品質の映像制作（予算・時間に余裕あり）
- **第1選択**: Hunyuan Video
- **第2選択**: Mochi 1
- **理由**: 映画的品質、プロフェッショナル用途に最適

### バランス重視（品質とVRAM）
- **第1選択**: Mochi 1
- **第2選択**: CogVideoX 5B
- **理由**: 12-24GB VRAMで高品質生成

### 低スペックPC（VRAM 8-12GB）
- **第1選択**: CogVideoX 2B
- **第2選択**: AnimateDiff
- **理由**: 低VRAM要件、実用的な品質

### 画像→動画変換
- **第1選択**: Stable Video Diffusion
- **第2選択**: CogVideoX 5B（img2vid）
- **理由**: 画像→動画に特化

### 動画スタイル変換（アニメ化など）
- **第1選択**: AnimateDiff（video-to-video）
- **理由**: 豊富なスタイルモジュール、ControlNet統合

### 高速プロトタイピング
- **第1選択**: LTX-Video
- **第2選択**: AnimateDiff
- **理由**: 生成速度が速い

---

## 12.5 ローカル動画生成ツール

### ComfyUI（最推奨）

**特徴**:
- **ノードベースワークフロー**: 複雑な動画生成パイプラインの構築
- **最新モデル対応**: Hunyuan Video、Mochi 1、CogVideoX、AnimateDiff全て対応
- **拡張性**: ControlNet、IPAdapter、アップスケーリング等の統合
- **ワークフロー共有**: コミュニティで多数のワークフローが公開

**推奨ユーザー**:
- プロフェッショナルなビデオクリエイター
- 高度なカスタマイズが必要なユーザー
- 複数モデル・エフェクトを組み合わせたい人

**主要カスタムノード**:
- ComfyUI-AnimateDiff-Evolved
- ComfyUI-CogVideoXWrapper
- ComfyUI-MochiWrapper
- ComfyUI-VideoHelperSuite（動画入出力）

**セットアップ**:
```bash
git clone https://github.com/comfyanonymous/ComfyUI.git
cd ComfyUI
pip install -r requirements.txt

# カスタムノードのインストール
cd custom_nodes
git clone https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git
git clone https://github.com/kijai/ComfyUI-CogVideoXWrapper.git
git clone https://github.com/kijai/ComfyUI-MochiWrapper.git
git clone https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite.git

# 各ノードの依存関係をインストール
cd ComfyUI-AnimateDiff-Evolved && pip install -r requirements.txt && cd ..
cd ComfyUI-CogVideoXWrapper && pip install -r requirements.txt && cd ..
cd ComfyUI-MochiWrapper && pip install -r requirements.txt && cd ..
```

---

### Automatic1111 + AnimateDiff拡張

**特徴**:
- **使い慣れたUI**: 画像生成と同じインターフェース
- **AnimateDiff特化**: 主にAnimateDiff用
- **初心者向け**: シンプルな設定

**推奨ユーザー**: Automatic1111を既に使用している初心者

**制限**:
- 最新モデル（Mochi 1、Hunyuan Videoなど）の対応が遅い
- ワークフローのカスタマイズ性が低い

---

### Diffusers（プログラマティック）

**特徴**:
- **Pythonコード**: Hugging Faceのdiffusersライブラリ
- **スクリプト制御**: バッチ処理・自動化に最適
- **最新モデル**: 公式リリース直後から利用可能

**推奨ユーザー**: 開発者、自動化が必要なユーザー

**セットアップ例**:
```python
from diffusers import CogVideoXPipeline
import torch

pipe = CogVideoXPipeline.from_pretrained(
    "THUDM/CogVideoX-5b",
    torch_dtype=torch.float16
).to("cuda")

video = pipe(
    prompt="A serene lake at sunset with mountains in the background",
    num_frames=48,
    guidance_scale=7.5
).frames

# 動画として保存
from diffusers.utils import export_to_video
export_to_video(video, "output.mp4", fps=8)
```

---

## 12.6 パフォーマンスとVRAM最適化

### 量子化によるVRAM削減

**FP16 → FP8 量子化**:
- VRAM使用量: 約50%削減
- 品質低下: 軽微
- 対応モデル: CogVideoX、Mochi 1

**4-bit/8-bit量子化（bitsandbytes）**:
- VRAM使用量: 60-75%削減
- 品質低下: 中程度（8-bitは実用的、4-bitは品質低下大）
- 対応: CogVideoX、Hunyuan Video

**例**: Hunyuan Video 13B（FP16: 60GB → FP8: 30GB → 8-bit: 15GB）

---

### フレーム数・解像度の調整

**VRAM削減テクニック**:
- フレーム数を減らす（48フレーム → 24フレーム）
- 解像度を下げる（720p → 480p）
- バッチサイズ1に固定

**推奨設定（12GB VRAM）**:
- CogVideoX 2B: 480p、24-32フレーム
- AnimateDiff: 512×512、16-24フレーム

---

### Gradient Checkpointing（勾配チェックポイント）

- メモリ使用量を削減（推論では不要、学習時のみ）
- 推論速度は変わらず

---

### モデルオフロード（CPU ↔ GPU）

**ComfyUIの低VRAMモード**:
```bash
python main.py --lowvram  # GPU/RAM間でモデルを移動
python main.py --novram   # モデルをRAMに保持、推論時のみGPU
```

**効果**:
- VRAM要件を大幅削減（60GB → 12GBなど）
- 推論速度は2-5倍遅くなる

---

## 12.7 コスト比較：クラウド vs ローカル

### クラウドサービスの料金（2025年10月）

| サービス | 料金プラン | 月額コスト | 制限 |
|----------|------------|------------|------|
| Runway Gen-3 | Standard | $12 (¥1,800) | 月間125秒（625クレジット） |
| Runway Gen-3 | Unlimited | $76 (¥11,400) | 無制限（低速モード） |
| Pika 1.5 | Standard | $10 (¥1,500) | 月間250クレジット |
| Pika 1.5 | Pro | $35 (¥5,250) | 月間750クレジット |
| Luma Dream Machine | Free | ¥0 | 月間30動画 |
| Luma Dream Machine | Standard | $30 (¥4,500) | 月間120動画 |
| Kling AI | Standard | $15 (¥2,250) | 月間66クレジット |

**従量課金換算**:
- Runway Gen-3: 約¥100/秒
- Pika 1.5: 約¥60/動画（4秒想定）

---

### ローカル環境の初期投資

| 構成 | 主要GPU | 初期費用（概算） | 月額コスト（電気代） | 1年総額 | 3年総額 |
|------|---------|------------------|----------------------|---------|---------|
| 最小 | RTX 4060 Ti 16GB | ¥70,000 | ¥400 | ¥74,800 | ¥84,400 |
| 標準 | RTX 4070 Ti 16GB | ¥120,000 | ¥500 | ¥126,000 | ¥138,000 |
| 推奨 | RTX 4090 24GB | ¥350,000 | ¥800 | ¥359,600 | ¥378,800 |
| ハイエンド | RTX A6000 48GB | ¥800,000 | ¥1,000 | ¥812,000 | ¥836,000 |

---

### 損益分岐点分析

**月間100動画生成の場合（1動画=5秒想定）**:

| 選択肢 | 初期費用 | 月額コスト | 1年総額 | 3年総額 |
|--------|----------|------------|---------|---------|
| Runway Unlimited | ¥0 | ¥11,400 | ¥136,800 | ¥410,400 |
| Luma Standard | ¥0 | ¥4,500 | ¥54,000 | ¥162,000 |
| ローカル（標準） | ¥120,000 | ¥500 | ¥126,000 | ¥138,000 |
| ローカル（推奨） | ¥350,000 | ¥800 | ¥359,600 | ¥378,800 |

**結論**:
- **1年でペイ**: 標準構成（RTX 4070 Ti）vs Runway Unlimited
- **2-3年でペイ**: 推奨構成（RTX 4090）vs Runway Unlimited
- 月間100動画以上生成する場合、ローカル環境が有利

---

### 月間500動画生成の場合（ヘビーユーザー）

| 選択肢 | 月額コスト | 1年総額 | 3年総額 |
|--------|------------|---------|---------|
| Runway Unlimited | ¥11,400 | ¥136,800 | ¥410,400 |
| Pika Pro（複数アカウント） | ¥21,000* | ¥252,000 | ¥756,000 |
| ローカル（推奨） | ¥800 | ¥359,600 | ¥378,800 |

*Pika Proは月間750クレジット（約187動画）が上限のため、500動画には複数アカウントが必要

**結論**: ヘビーユーザーの場合、RTX 4090構成でも**2-3年で大幅な節約**

---

## 12.8 推奨構成まとめ

### 初心者向け（予算・学習重視）

**ハードウェア**:
- GPU: RTX 4060 Ti 16GB（¥70,000）
- RAM: 32GB
- ストレージ: 512GB NVMe SSD

**ソフトウェア**:
- ツール: ComfyUI
- モデル: AnimateDiff、CogVideoX 2B

**想定コスト**: ¥70,000（初期）+ ¥400/月（電気代）

---

### 中級者向け（バランス型）

**ハードウェア**:
- GPU: RTX 4070 Ti 16GB（¥120,000）
- RAM: 64GB
- ストレージ: 1TB NVMe SSD

**ソフトウェア**:
- ツール: ComfyUI
- モデル: CogVideoX 5B、Mochi 1（量子化版）

**想定コスト**: ¥120,000（初期）+ ¥500/月（電気代）

---

### プロフェッショナル向け（品質重視）

**ハードウェア**:
- GPU: RTX 4090 24GB（¥350,000）
- RAM: 128GB
- ストレージ: 2TB NVMe SSD

**ソフトウェア**:
- ツール: ComfyUI
- モデル: Mochi 1、CogVideoX 5B、Hunyuan Video（量子化版）

**想定コスト**: ¥350,000（初期）+ ¥800/月（電気代）

---

### スタジオ・商用向け（最高品質）

**ハードウェア**:
- GPU: RTX A6000 48GB または A100 80GB（¥800,000+）
- RAM: 256GB
- ストレージ: 4TB NVMe RAID

**ソフトウェア**:
- ツール: ComfyUI、カスタムパイプライン
- モデル: Hunyuan Video（FP16完全版）、全モデル対応

**想定コスト**: ¥800,000+（初期）+ ¥1,000/月（電気代）

---

## 12.9 セットアップガイド

### Step 1: ComfyUIのインストール

```bash
# ComfyUIのクローン
git clone https://github.com/comfyanonymous/ComfyUI.git
cd ComfyUI
pip install -r requirements.txt
```

---

### Step 2: 動画生成用カスタムノードのインストール

```bash
cd custom_nodes

# AnimateDiff Evolved
git clone https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git
cd ComfyUI-AnimateDiff-Evolved
pip install -r requirements.txt
cd ..

# CogVideoX Wrapper
git clone https://github.com/kijai/ComfyUI-CogVideoXWrapper.git
cd ComfyUI-CogVideoXWrapper
pip install -r requirements.txt
cd ..

# Mochi Wrapper
git clone https://github.com/kijai/ComfyUI-MochiWrapper.git
cd ComfyUI-MochiWrapper
pip install -r requirements.txt
cd ..

# Video Helper Suite（動画入出力）
git clone https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite.git
cd ComfyUI-VideoHelperSuite
pip install -r requirements.txt
cd ..
```

---

### Step 3: モデルのダウンロード

#### AnimateDiff

```bash
cd ComfyUI/custom_nodes/ComfyUI-AnimateDiff-Evolved/models

# モーションモジュール v2（推奨）
wget https://huggingface.co/guoyww/animatediff/resolve/main/mm_sd_v15_v2.ckpt

# モーションモジュール v3
wget https://huggingface.co/guoyww/animatediff/resolve/main/v3_sd15_mm.ckpt
```

#### CogVideoX 5B

```bash
# Hugging Face CLIでダウンロード
pip install huggingface_hub
huggingface-cli download THUDM/CogVideoX-5b --local-dir ComfyUI/models/CogVideoX/CogVideoX-5b
```

#### Mochi 1

```bash
cd ComfyUI/models

# FP8量子化版（推奨、12GB VRAMで動作）
wget https://huggingface.co/genmo/mochi-1-preview/resolve/main/mochi_preview_dit_fp8_e4m3fn.safetensors

# VAE
wget https://huggingface.co/genmo/mochi-1-preview/resolve/main/decoder.safetensors
```

---

### Step 4: ComfyUIの起動

```bash
cd ComfyUI
python main.py

# 低VRAMモードで起動（12GB以下のGPU）
python main.py --lowvram

# ブラウザで http://127.0.0.1:8188 にアクセス
```

---

### Step 5: サンプルワークフローの読み込み

1. ブラウザでComfyUIにアクセス
2. 右上の「Load」ボタンをクリック
3. `custom_nodes/ComfyUI-AnimateDiff-Evolved/workflows/` 内のワークフローを選択
4. プロンプトを入力
5. 「Queue Prompt」をクリックして生成開始

---

### トラブルシューティング

**CUDA Out of Memory エラー**:
```bash
# 低VRAMモードで起動
python main.py --lowvram

# または超低VRAMモード（RAM使用）
python main.py --novram
```

**動画生成が途中で止まる**:
- フレーム数を減らす（48 → 24フレーム）
- 解像度を下げる（720p → 480p）
- バッチサイズを1にする

**ffmpegエラー**:
```bash
# ffmpegのインストール（Ubuntu/Debian）
sudo apt install ffmpeg

# macOS
brew install ffmpeg

# Windows: https://ffmpeg.org/download.html からダウンロード
```

---

## 12.10 最適なローカル動画生成モデル選択フローチャート

```
【予算はいくら？】
├─ ¥70,000以下
│  └─ GPU: RTX 4060 Ti 16GB
│     ├─ ツール: ComfyUI
│     └─ モデル: AnimateDiff、CogVideoX 2B
│
├─ ¥120,000前後
│  └─ GPU: RTX 4070 Ti 16GB ← ★バランス型
│     ├─ ツール: ComfyUI
│     └─ モデル: CogVideoX 5B、Mochi 1（量子化）
│
├─ ¥350,000前後
│  └─ GPU: RTX 4090 24GB ← ★プロ向け
│     ├─ ツール: ComfyUI
│     └─ モデル: Mochi 1、Hunyuan Video（量子化）
│
└─ ¥800,000以上
   └─ GPU: RTX A6000 48GB / A100 80GB
      ├─ ツール: ComfyUI、カスタムパイプライン
      └─ モデル: Hunyuan Video（完全版）

【用途別の選択】
├─ 最高品質の映像制作 → Hunyuan Video、Mochi 1
├─ バランス重視（品質×VRAM） → Mochi 1、CogVideoX 5B
├─ 低スペックPC（8-12GB） → CogVideoX 2B、AnimateDiff
├─ 画像→動画変換 → Stable Video Diffusion、CogVideoX img2vid
├─ 動画スタイル変換 → AnimateDiff（video-to-video）
└─ 高速プロトタイピング → LTX-Video

【品質レベル別】
├─ 映画・CM品質 → Hunyuan Video ⭐⭐⭐⭐⭐
├─ プロフェッショナル → Mochi 1 ⭐⭐⭐⭐⭐
├─ 高品質（SNS用） → CogVideoX 5B ⭐⭐⭐⭐
├─ 標準品質 → CogVideoX 2B ⭐⭐⭐
└─ 実験・学習用 → AnimateDiff ⭐⭐⭐
```

---

## まとめ

ローカル動画生成AIは、2025年現在、**Hunyuan Video**、**Mochi 1**、**CogVideoX**を中心に急速に進化しています。

**推奨構成**:
- **初心者・予算重視**: RTX 4060 Ti 16GB + ComfyUI + AnimateDiff/CogVideoX 2B
- **中級者・バランス型**: RTX 4070 Ti 16GB + ComfyUI + CogVideoX 5B/Mochi 1（量子化）
- **プロ・品質重視**: RTX 4090 24GB + ComfyUI + Mochi 1/Hunyuan Video（量子化）

**コスト面**:
- 月間100動画以上生成する場合、**1-2年でローカル環境が有利**
- ヘビーユーザー（月間500動画）なら**圧倒的にローカルが安い**

**品質面**:
- **Hunyuan Video**: 映画的品質、物理精度が最高レベル ⭐⭐⭐⭐⭐
- **Mochi 1**: オープンソース最高品質、プロンプト精度に優れる ⭐⭐⭐⭐⭐
- **CogVideoX**: 実用性重視、VRAM要件が現実的 ⭐⭐⭐⭐

**課題**:
- VRAM要件が高い（最低12GB、理想は24GB+）
- 生成時間が長い（数分〜数十分）
- 動画の長さが5-10秒程度に制限される

ローカル動画生成AIは、まだ発展途上ですが、2025年以降、量子化技術の進化とGPUの低価格化により、より多くのユーザーにとって実用的な選択肢となるでしょう。
