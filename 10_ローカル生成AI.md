# 第10章: ローカル生成AI - コーディング特化モデルカタログ

プライバシー、コスト、オフライン利用を重視する開発者にとって、ローカル環境で動作するLLMは重要な選択肢です。本章では、コーディング用途に特化したローカルLLMを、マシンスペックと目的に応じて選択できるよう体系的にカタログ化します。

## 10.1 ローカルLLMの概要とメリット

### なぜローカルLLMなのか？

```
クラウドAI                vs              ローカルAI
  ↓                                         ↓
● 高性能                              ● プライバシー保護
● 常に最新                            ● コスト削減（従量課金なし）
● セットアップ不要                    ● オフライン利用可能
● 月額/従量課金                       ● カスタマイズ自由
                                      ● 企業コード流出リスクゼロ
```

### ローカルLLMが最適なケース

✅ **プライバシー重視**
- 企業の機密コードを外部に送信したくない
- セキュリティポリシーでクラウドAI利用が制限される

✅ **コスト削減**
- 大量のコード生成で従量課金が高額になる
- 初期投資後はランニングコストなし

✅ **オフライン環境**
- インターネット接続が不安定
- 機内、移動中でも作業したい

✅ **カスタマイズ**
- 特定のコーディングスタイルに特化させたい
- 社内フレームワーク用にファインチューニング

### 2025年のローカルLLMの進化

2025年のローカルLLMコーディング環境は大きく成熟しました：

- **性能向上**: Qwen3-CoderはClaude Sonnet 4に匹敵
- **小型化**: 1.5B〜7Bモデルでも実用的な性能
- **ツールの充実**: Ollama, LM Studioで簡単セットアップ
- **量子化技術**: 4bit/8bit量子化でVRAM消費を大幅削減

## 10.2 マシンスペック別推奨モデル

### スペック分類とVRAM要件

| スペック | VRAM | 推奨モデルサイズ | 快適度 | 想定ハードウェア |
|---------|------|----------------|--------|----------------|
| **エントリー** | 4-8GB | 1.5B-3B (Q4) | ⭐⭐ | GTX 1660, RTX 3050 |
| **ミドル** | 12-16GB | 7B-8B (Q5-Q6) | ⭐⭐⭐⭐ | RTX 3060, RTX 4060 Ti |
| **ハイエンド** | 24GB | 14B-32B (Q4-Q5) | ⭐⭐⭐⭐⭐ | RTX 3090, RTX 4090 |
| **プロ/ワークステーション** | 40GB+ | 70B+ (Q4) | ⭐⭐⭐⭐⭐ | RTX 4090×2, A100 |

### メモリ計算式

**基本式**:
```
必要VRAM (GB) ≈ モデルパラメータ数 (B) × 量子化ビット数 / 8 × 1.2

例: 7B Q4モデル
= 7 × 4 / 8 × 1.2 = 4.2GB
```

**実用的な目安**:
- 7B Q4: 約4-5GB
- 14B Q4: 約8-9GB
- 32B Q4: 約18-20GB
- 70B Q4: 約42-48GB

### スペック別推奨構成（2025年版）

#### エントリー構成（予算15万円）
```
GPU: RTX 4060 (8GB) - ¥40,000
CPU: Ryzen 5 7600 - ¥30,000
RAM: DDR5 32GB - ¥15,000
SSD: 1TB NVMe - ¥10,000
その他: ¥55,000

推奨モデル: Qwen3-Coder 3B, Phi-4 Mini
```

#### ミドル構成（予算25万円）
```
GPU: RTX 4070 (12GB) - ¥80,000
CPU: Ryzen 7 7700X - ¥45,000
RAM: DDR5 64GB - ¥30,000
SSD: 2TB NVMe - ¥15,000
その他: ¥80,000

推奨モデル: Qwen3-Coder 7B, DeepSeek Coder V2 6.7B
```

#### ハイエンド構成（予算50万円）
```
GPU: RTX 4090 (24GB) - ¥280,000
CPU: Ryzen 9 7950X - ¥80,000
RAM: DDR5 128GB - ¥60,000
SSD: 4TB NVMe - ¥30,000
その他: ¥50,000

推奨モデル: Qwen3-Coder 32B, DeepSeek R1-0528 14B, Llama 3.3 70B (Q4)
```

#### プロ構成（予算100万円+）
```
GPU: RTX 4090 × 2 (48GB total) - ¥560,000
または RTX 5080 32GB × 2 (64GB total)
CPU: Ryzen Threadripper - ¥150,000
RAM: DDR5 256GB - ¥120,000
SSD: 8TB NVMe - ¥60,000
その他: ¥110,000

推奨モデル: Qwen3-Coder 72B, DeepSeek V3.1, Llama 4 70B (Q6)
```

#### 🆕 コンパクトワークステーション構成（AMD Strix Halo）

**MINISFORUM MS-S1 MAX** ¥345,000 (約$2,299)

```
APU: AMD Ryzen AI Max+ 395 (16コアZen 5)
統合GPU: Radeon 8060S (40 RDNA 3.5 CU、最大96GB VRAM利用可)
NPU: XDNA 2 (50 TOPS) + 総合126 TOPS AI性能
RAM: 128GB LPDDR5X-8000 (256GB/s帯域幅)
TDP: 110-160W（4段階調整可能）
電源: 320W内蔵PSU
拡張: PCIe 4.0 x16スロット（x4配線）、デュアルUSB4 V2 (80Gbps)
サイズ: ミニPC（超コンパクト）

特徴:
✅ 70B〜100Bパラメータモデルをローカル実行可能
✅ 統合GPUで96GBまでVRAM利用（業界最大級）
✅ 2台クラスタで235B Q4モデルが10.87 tok/sec
✅ デスクトップ級GPU性能（RTX 4060相当）
✅ 省スペース・低消費電力（160W MAX）

推奨モデル:
- Qwen3-Coder 72B Q4
- Llama 3.3 70B Q4-Q5
- DeepSeek Coder V2 33B Q6
- Gemma 3 27B Q6

評価: ⭐⭐⭐⭐⭐ - コンパクトで超大型LLM実行可能
```

**Strix Haloが最適なケース**:
- 省スペースで高性能が必要
- 70B以上のモデルを動かしたいが、デュアルGPU構成は避けたい
- 電力効率を重視
- 将来的にクラスタ構成を検討

**注意点**:
- 専用GPUと比較してやや低速（RTX 4090比で30-40%低速）
- メモリ帯域がボトルネックになる場合あり
- 日本での入手性は要確認

---

## 10.3 コーディング特化モデルカタログ（2025年版）

### 総合性能ランキング（2025年10月時点）

| 順位 | モデル | 開発元 | パラメータ | HumanEval | LiveCodeBench / SWE-bench | 総合評価 |
|-----|--------|--------|-----------|------------|------------------------|---------|
| 🥇 1 | **Qwen3-Coder** | Alibaba | 3B-480B | 88.3% (7B) | 47.2% (235B) | ⭐⭐⭐⭐⭐ |
| 🥈 2 | **DeepSeek V3.1** | DeepSeek | 671B (37B active) | - | 66.0% SWE-bench | ⭐⭐⭐⭐⭐ |
| 🥉 3 | **DeepSeek R1-0528** | DeepSeek | 1.5B-70B | - | Codeforces ~1930 | ⭐⭐⭐⭐⭐ |
| 4 | **Phi-4** | Microsoft | 3.8B-14B | 82.6 (14B) | 53.8% (14B) | ⭐⭐⭐⭐⭐ |
| 5 | **DeepSeek Coder V2** | DeepSeek | 6.7B-236B | - | 37.6% | ⭐⭐⭐⭐⭐ |
| 6 | **Gemma 3** | Google | 1B-27B | ⭐高 | ⭐高 | ⭐⭐⭐⭐⭐ |
| 7 | **Llama 4** | Meta | 70B+ MoE | - | マルチモーダル | ⭐⭐⭐⭐ |
| 8 | **StarCoder2** | BigCode | 3B-15B | 33B+級性能 | - | ⭐⭐⭐⭐ |
| 9 | **Gemma 3n** | Google | E2B/E4B | モバイル最強 | - | ⭐⭐⭐⭐ |

### 詳細モデルカタログ

---

#### 🏆 Qwen3-Coder（最推奨）

**開発元**: Alibaba Qwen Team
**リリース**: 2025年7月22日
**ライセンス**: Apache 2.0

**主要モデル**:
| モデル | Total Params | Active Params | VRAM (Q4) | 用途 |
|--------|--------------|---------------|-----------|------|
| **Qwen3-Coder-480B-A35B** | 480B | 35B | 90GB | 最高性能・エンタープライズ |
| **Qwen3-Coder-Flash (30B-A3B)** | 30.5B | 3.3B | 8GB | 高速・実用バランス |
| **Qwen3-32B** | 32B | 32B | 20GB | 密モデル・高性能 |
| **Qwen3-14B** | 14B | 14B | 9GB | ミドル〜ハイエンド |
| **Qwen3-7B** | 7B | 7B | 4.5GB | 標準・バランス |
| **Qwen3-3B** | 3B | 3B | 2.5GB | 軽量・エントリー |

**革新的な特徴**:
- **MoE (Mixture-of-Experts)**: 160 experts、8 activated per inference
- **超大規模コンテキスト**: 256K tokens native、1M tokens拡張可能
- **7.5T トークン学習**: コード比率70%の大規模データセット
- **LiveCodeBench 47.2**: Qwen 2.5の38.7から22%向上
- **エージェント型コーディング**: Claude Sonnet級の性能

**ベンチマーク**:
- HumanEval: 88.3% (7B)
- LiveCodeBench: 47.2% (235B)
- Aider-Edit: 84.2%+
- 100+プログラミング言語対応

**推奨量子化**:
- 8GB VRAM: 3B-Q4 または Flash 30B-A3B Q4
- 12GB VRAM: 7B-Q5 または 14B-Q4
- 24GB VRAM: 32B-Q4
- 48GB+ VRAM: 480B-A35B Q4

**Ollamaでの使用**:
```bash
# Qwen3-Coder 7B (推奨)
ollama pull qwen3-coder:7b-instruct-q4_K_M

# Qwen3-Coder 32B (高性能)
ollama run qwen3-coder:32b-instruct-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - 2025年の最強コーディングLLM（Claude Sonnet 4に匹敵）

---

#### 🔥 DeepSeek R1-0528（推論特化）

**開発元**: DeepSeek
**リリース**: 2025年5月28日
**ライセンス**: MIT

**特徴**:
- **Chain-of-Thought推論**: 推論プロセスを可視化
- **Codeforces ~1930**: 従来の~1530から+400ポイント向上
- 数学・論理的思考に強い
- 日本語版あり（Lightblue/サイバーエージェント）

**モデルバリエーション**:
| サイズ | VRAM (Q4) | Codeforces | 特徴 |
|--------|----------|-----------|------|
| 1.5B | 1.5GB | - | 軽量版 |
| 7B | 4.5GB | - | 標準版 |
| 14B | 9GB | ~1930 | ベストバランス |
| 70B | 45GB | - | 最高性能 |

**強み**:
- 複雑なアルゴリズム設計（Codeforces競技プログラミングレベル）
- デバッグ支援（思考過程をステップバイステップ表示）
- アーキテクチャ設計の提案
- O3、Gemini 2.5 Proに接近する性能

**ベンチマーク（R1-0528）**:
- Codeforces: ~1930 (従来比+400ポイント)
- 数学、コーディング、推論で大幅向上

**Ollamaでの使用**:
```bash
# DeepSeek R1-0528 14B (推奨)
ollama pull deepseek-r1:14b-qwen-distill-q4_K_M

# 日本語特化版
ollama pull lightblue/deepseek-r1-distill-qwen-14b-japanese
```

**評価**: ⭐⭐⭐⭐⭐ - 推論タスク最強（2025年5月アップデート版）

---

#### 🌊 DeepSeek V3.1（ハイブリッド思考モデル）

**開発元**: DeepSeek
**リリース**: 2025年8月19日
**ライセンス**: MIT

**特徴**:
- **ハイブリッドアーキテクチャ**: V3とR1を統合した単一モデル
- **思考モード切替**: チャットテンプレートで「思考モード」と「非思考モード」を切り替え可能
- **671B Total / 37B Active**: MoEアーキテクチャで高効率

**モデルスペック**:
| パラメータ | VRAM (Q4) | コンテキスト | 特徴 |
|----------|----------|-------------|------|
| 671B (37B active) | 約100GB | 128K tokens | 思考モード搭載 |

**強み**:
- **SWE-bench Verified 66.0%**: DeepSeek R1の44.6%を大幅超え
- **Aider Polyglot 71.6%**: Claude 4 Opusを上回る
- **プログラミング特化**: マルチ言語コード生成で業界トップクラス
- DeepSeek R1の推論能力 + V3の高速性を同時に実現

**ベンチマーク**:
- SWE-bench Verified: 66.0% (R1-0528: 44.6%)
- Aider Polyglot: 71.6%
- BF16、FP8、F32対応

**評価**: ⭐⭐⭐⭐⭐ - 思考と速度の両立（2025年8月の最新ハイブリッドモデル）

---

#### 🚀 DeepSeek Coder V2

**開発元**: DeepSeek
**リリース**: 2024年

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 特徴 |
|--------|----------|------|
| 6.7B | 4.2GB | コンパクト高性能 |
| 33B | 20GB | ハイエンド |
| 236B MoE | 90GB | 最大性能版 |

**強み**:
- Fill-in-the-Middle対応（カーソル位置でのコード補完）
- **338プログラミング言語対応**（業界最多クラス）
- レポジトリレベルのコード理解
- 236B MoEバージョンで超大規模コードベース対応

**システム要件**:
- 6.7B Lite: 40GB VRAM (BF16) / 10.4GB (Q4)
- 236B MoE: 推奨90GB+ VRAM

**評価**: ⭐⭐⭐⭐⭐ - コード補完に特化

---

#### 🦙 Llama 4（Meta最新・マルチモーダルMoE）

**開発元**: Meta
**リリース**: 2025年4月5日
**ライセンス**: Llama 4 License

**モデルバリエーション**:
| モデル | パラメータ | 特徴 | ステータス |
|--------|----------|------|-----------|
| **Llama 4 Scout** | 小型MoE | 軽量・高速 | ✅ リリース済み |
| **Llama 4 Maverick** | 中型MoE | バランス型 | ✅ リリース済み |
| **Llama 4 Behemoth** | 超大型MoE | 最高性能 | 🚧 トレーニング中 |

**革新的な特徴**:
- **Mixture-of-Experts (MoE)**: Llamaシリーズ初のMoEアーキテクチャ
- **マルチモーダル**: テキスト、画像、動画を統合処理
- **12言語対応**: 多言語コーディング支援
- 音声・推論機能の大幅強化

**強み**:
- 汎用LLMとしての高い性能（コーディング以外も強い）
- マルチモーダルでUI/UX設計にも活用可能
- Metaの最先端技術を凝縮

**注意**:
- コーディング特化ではQwen3-Coderが上位
- Behemothは未リリース（2025年10月時点）
- CodeLlamaシリーズは事実上、Llama 4に統合

**評価**: ⭐⭐⭐⭐ - マルチモーダル対応の汎用LLM（コーディング特化ではない）

---

#### 🔵 Phi-4（最新 - コーディング特化）

**開発元**: Microsoft
**リリース**: 2025年1月
**ライセンス**: MIT

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 特徴 |
|--------|----------|------|
| Phi-4-Mini (3.8B) | 2.5GB | 軽量版 |
| Phi-4 (14B) | 9GB | 標準版 |
| Phi-4-reasoning (14B) | 9GB | 推論特化 |

**強み**:
- **HumanEval 82.6**: LLaMA 3.3 70Bを上回る驚異的なスコア
- **LiveCodeBench 53.8%**: コーディング性能が大幅向上（+25ポイント）
- **MATH 80.4**: 数学的推論も優秀
- 14Bで70Bクラスの性能を実現

**ベンチマーク**:
- HumanEval: 82.6
- LiveCodeBench: 53.8%
- MATH: 80.4
- GSM-8K: 88.6% (Mini版)

**推奨用途**:
- 中スペックGPU（12-16GB）でのハイパフォーマンス
- 数学・論理的コーディングタスク
- コンパクトながら高品質な生成が必要な場合

**Ollamaでの使用**:
```bash
# Phi-4 14B (推奨)
ollama pull phi4:14b-q4_K_M

# Phi-4 Mini 3.8B (軽量版)
ollama pull phi4:mini-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - 14Bで最高クラスのコーディング性能

---

#### ⚡ Phi-3 Mini（旧世代）

**開発元**: Microsoft
**サイズ**: 3.8B
**VRAM**: 2.5GB (Q4)

**強み**:
- 非常に軽量
- GPUなしでもCPUで動作可能
- モバイルデバイス対応

**用途**:
- 低スペックPC
- ラップトップでの軽量補完
- 学習用

**注意**: Phi-4 Miniの登場により、新規導入は推奨しない。

**評価**: ⭐⭐⭐ - 軽量だが性能は控えめ（Phi-4への移行推奨）

---

#### 🔷 Gemma 3（Google最新・マルチモーダル）

**開発元**: Google / DeepMind
**リリース**: 2025年3月
**ライセンス**: Gemma License（商用可）
**ベース技術**: Gemini 2.0と同じ研究・技術

**モデルバリエーション**:
| サイズ | VRAM (Q4) | マルチモーダル | コンテキスト | 特徴 |
|--------|----------|--------------|-------------|------|
| 270M | 0.5GB | テキストのみ | - | 超軽量ファインチューニング用 |
| 1B | 1GB | テキストのみ | 128K | モバイル向け |
| 4B | 3GB | 画像+テキスト | 128K | バランス型 |
| 12B | 8GB | 画像+テキスト | 128K | 高性能 |
| 27B | 17GB | 画像+テキスト | 128K | 最高性能 |

**強み**:
- **マルチモーダル**: 画像とテキストを同時処理（4B以上）
- **大規模コンテキスト**: 128K tokens（約300ページの本、30枚の高解像度画像、1時間以上の動画）
- **Gemini 2.0ベース**: Googleの最先端技術を凝縮
- コーディング、数学、推論で優れた性能
- 140+言語対応

**ベンチマーク**:
- HumanEval・MBPPで高スコア
- 27BはA100 80GB・H100で効率的に動作
- 世界最高のシングルアクセラレータモデル（27B）

**推奨用途**:
- マルチモーダルコーディング（コード + 図解）
- 多言語コード生成
- 数学・科学系コーディング
- モバイル/エッジデバイス（1B、270M）

**Ollamaでの使用**:
```bash
# Gemma 3 12B (推奨バランス型)
ollama pull gemma3:12b-instruct-q4_K_M

# Gemma 3 27B (高性能)
ollama pull gemma3:27b-instruct-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - Gemini 2.0技術のオープン版

---

#### 📱 Gemma 3n（モバイル特化・効率重視）

**開発元**: Google / DeepMind
**リリース**: 2025年6月
**ライセンス**: Gemma License（商用可）
**特徴**: Gemini Nanoと同じアーキテクチャ

**モデルバリエーション**:
| モデル | 実際のパラメータ | 実効メモリ | マルチモーダル |
|--------|----------------|-----------|--------------|
| E2B | 5B | 2GB | 音声+画像+テキスト+動画 |
| E4B | 8B | 3GB | 音声+画像+テキスト+動画 |

**革新的アーキテクチャ**:
- **Matryoshka Transformer (MatFormer)**: 大きなモデル内に小さなモデルをネスト
- **Per-Layer Embedding (PLE)**: レイヤーごとの埋め込みでパフォーマンス向上
- **MobileNet-V5-300M**: 新世代ビジョンエンコーダー
- **音声処理**: 30秒までの音声クリップを処理（転写・翻訳）

**強み**:
- 5Bパラメータが2GBメモリで動作（従来の2Bモデル相当）
- モバイル・エッジデバイスで最高クラスの性能
- 音声、画像、動画、テキストを統合処理

**推奨用途**:
- モバイルアプリ開発
- エッジAIコーディング支援
- 低メモリ環境での高性能推論
- マルチモーダルタスク

**評価**: ⭐⭐⭐⭐ - モバイル/エッジ最強モデル

---

#### 🌟 StarCoder2

**開発元**: BigCode
**サイズ**: 3B, 7B, 15B

**強み**:
- オープンソースコードで学習
- 透明性が高い
- コミュニティ主導

**評価**: ⭐⭐⭐⭐

---

#### 🦙 Llama 3.3

**開発元**: Meta
**サイズ**: 70B
**VRAM**: 45GB (Q4)

**特徴**:
- 汎用LLMとしての性能
- コーディング以外も強い
- 推論能力向上

**評価**: ⭐⭐⭐⭐ - 汎用性は高いがコーディング特化には劣る

---

## 10.4 用途別モデル選択ガイド

### シーン1: 日常的なコード補完

**目的**: VS Codeでリアルタイム補完

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| エントリー | Qwen3-Coder 3B Q4 | 軽量で高速 |
| ミドル | Qwen3-Coder 7B Q5 | ベストバランス |
| ハイエンド | Qwen3-Coder 14B Q4 | 最高品質 |

---

### シーン2: 複雑なアルゴリズム設計

**目的**: データ構造やアルゴリズムの実装

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | DeepSeek R1-0528 7B | 推論過程が見える |
| ハイエンド | DeepSeek R1-0528 14B | 最適 |
| プロ | Qwen3-Coder 32B | 最高性能 |

---

### シーン3: リファクタリング

**目的**: 既存コードの改善

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | Qwen3-Coder 7B Q6 | コードベース理解 |
| ハイエンド | Qwen3-Coder 32B Q4 | 大規模リファクタリング |

---

### シーン4: デバッグ支援

**目的**: バグの原因特定と修正

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | DeepSeek R1-0528 7B | 思考過程でバグ特定 |
| ハイエンド | DeepSeek R1-0528 14B | 複雑なバグ対応 |

---

### シーン5: プロジェクト全体の理解

**目的**: 大規模コードベースの分析

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ハイエンド | Qwen3-Coder 32B | 256Kコンテキスト |
| プロ | Qwen3-Coder 480B-A35B | 最大理解力・100万トークン対応 |

---

## 10.5 ローカルLLM実行ツール

### Ollama vs LM Studio 比較

| 特徴 | Ollama | LM Studio |
|------|--------|-----------|
| **インターフェース** | CLI + 簡易GUI | フルGUI |
| **使いやすさ** | 開発者向け | 初心者向け |
| **柔軟性** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| **API提供** | REST API標準 | あり |
| **対応OS** | Mac, Linux, Windows | Mac, Windows |
| **モデル管理** | CLI | GUI（ドラッグ&ドロップ） |
| **推奨用途** | 開発統合、自動化 | 手軽に試す、チャット |

### Ollama セットアップ（推奨）

#### インストール

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# https://ollama.com/download からインストーラーをダウンロード
```

#### モデルのダウンロードと実行

```bash
# Qwen3-Coder 7B (推奨)
ollama pull qwen3-coder:7b-instruct-q4_K_M
ollama run qwen3-coder:7b-instruct-q4_K_M

# Qwen3-Coder 32B (高性能)
ollama pull qwen3-coder:32b-instruct-q4_K_M

# DeepSeek R1-0528 14B
ollama pull deepseek-r1:14b-qwen-distill-q4_K_M
ollama run deepseek-r1:14b-qwen-distill-q4_K_M

# Phi-4 14B
ollama pull phi4:14b-q4_K_M
```

#### VS Code統合（Continue拡張）

```bash
# Continue拡張をインストール
# VS Code > Extensions > "Continue" で検索

# 設定例 (~/.continue/config.json)
{
  "models": [
    {
      "title": "Qwen3-Coder",
      "provider": "ollama",
      "model": "qwen3-coder:7b-instruct-q4_K_M"
    }
  ]
}
```

#### Cline拡張との連携

```bash
# Cline拡張をインストール
# VS Code > Extensions > "Cline" で検索

# Clineの設定でOllamaを選択
# モデル: qwen3-coder:7b-instruct-q4_K_M
# APIエンドポイント: http://localhost:11434
```

### LM Studio セットアップ

1. https://lmstudio.ai/ からダウンロード
2. アプリ起動
3. モデル検索（例: "qwen2.5-coder"）
4. ダウンロードボタンをクリック
5. チャットで使用

**メリット**: GUIで直感的、初心者に優しい
**デメリット**: CLI自動化には不向き

## 10.6 量子化とパフォーマンス

### 量子化レベルの選択

| 量子化 | VRAM削減率 | 品質 | 推奨用途 |
|--------|----------|------|---------|
| **Q2_K** | 75% | ⭐⭐ | 非推奨（劣化大） |
| **Q3_K_M** | 62.5% | ⭐⭐⭐ | VRAM極小時のみ |
| **Q4_K_M** | 50% | ⭐⭐⭐⭐ | **標準推奨** |
| **Q5_K_M** | 37.5% | ⭐⭐⭐⭐⭐ | バランス良好 |
| **Q6_K** | 25% | ⭐⭐⭐⭐⭐ | 高品質 |
| **Q8_0** | 0% | ⭐⭐⭐⭐⭐ | ほぼFP16 |
| **FP16** | - | ⭐⭐⭐⭐⭐ | 最高品質（重い） |

### 実用的な選び方

**12GB VRAM**:
- 7B Q5_K_M（高品質）
- 14B Q4_K_M（大型モデル）

**16GB VRAM**:
- 7B Q6_K（最高品質）
- 14B Q5_K_M（バランス）

**24GB VRAM**:
- 32B Q4_K_M（大型モデル）
- 14B FP16（最高品質）

## 10.7 コスト比較：クラウド vs ローカル

### 月間1000リクエスト（平均500トークン入出力）の場合

| 選択肢 | 初期費用 | 月額コスト | 1年総額 | 3年総額 | 備考 |
|--------|---------|-----------|---------|---------|------|
| **Claude Sonnet 4.5 API** | ¥0 | ¥9,000 | ¥108,000 | ¥324,000 | 従量課金 |
| **Claude Pro/Team** | ¥0 | ¥2,800-4,500 | ¥33,600-54,000 | ¥100,800-162,000 | **CLI利用可**、制限あり |
| **ChatGPT Plus** | ¥0 | ¥2,800 | ¥33,600 | ¥100,800 | 制限あり |
| **ChatGPT Pro** | ¥0 | ¥28,000 | ¥336,000 | ¥1,008,000 | 無制限GPT-4o/o1 |
| **OpenAI API契約** | ¥0 | 変動 | - | - | **Codex CLI利用可** |
| **Gemini Advanced** | ¥0 | ¥2,800 | ¥33,600 | ¥100,800 | 高い上限 |
| **Microsoft 365 Copilot** | ¥0 | ¥4,200 | ¥50,400 | ¥151,200 | $30/月 |
| **ローカル（ミドル）** | ¥250,000 | ¥500* | ¥256,000 | ¥268,000 | 無制限 |
| **ローカル（ハイエンド）** | ¥500,000 | ¥800* | ¥509,600 | ¥528,800 | 無制限 |

*電気代のみ（GPU稼働時150W想定、¥30/kWh、月100時間使用）

**💡 重要: Max/Proプランの隠れたメリット**

サブスクリプションプランには、CLIツールの利用が含まれる場合があります：

- **Claude Pro/Team**: Claude CLI（claude-code）を使用可能。月額プラン内で一定量のCLI利用が可能。コーディング支援ツールとして追加費用なしで利用できるため、実質的なコストパフォーマンスが向上。

- **OpenAI API契約**: API契約者は、Codex CLIやOpenAI Code Interpreterへのアクセスが提供される場合があります（契約内容による）。大量のAPI利用者には優遇条件が適用されることも。

- **利用シーン**: WebUIでのチャット＋CLIでのコーディング支援の併用が可能になるため、ローカルLLMとクラウドサービスのハイブリッド運用がコスト効率良く実現できます。

### 月間5000リクエスト（ヘビーユーザー）の場合

| 選択肢 | 月額コスト | 1年総額 | 3年総額 | 備考 |
|--------|-----------|---------|---------|------|
| **Claude Sonnet 4.5 API** | ¥45,000 | ¥540,000 | ¥1,620,000 | 従量課金 |
| **ChatGPT Pro** | ¥28,000 | ¥336,000 | ¥1,008,000 | 無制限（おすすめ） |
| **ローカル（ミドル）** | ¥1,500* | ¥268,000 | ¥286,000 | **最コスパ** |
| **ローカル（ハイエンド）** | ¥2,400* | ¥528,800 | ¥557,600 | 高性能 |

*電気代（月300時間使用）

### 損益分岐点

- **ミドル構成**: 約28ヶ月でClaude Sonnet 4.5と同等
- **ハイエンド構成**: 約56ヶ月でClaude Sonnet 4.5と同等

### ローカルが有利なケース

✅ 大量使用（月3,000リクエスト以上）
✅ 長期利用予定（2年以上）
✅ プライバシー重視
✅ 複数人で共有
✅ オフライン必須

### クラウドが有利なケース

✅ 低頻度使用（月500リクエスト以下）
✅ 初期投資を避けたい
✅ 最新モデルを常に使いたい
✅ メンテナンス不要

## 10.8 推奨構成まとめ

### 2025年ベストプラクティス

#### 【最推奨】ミドルレンジ構成

```
GPU: RTX 4070 12GB - ¥80,000
CPU: Ryzen 7 7700X - ¥45,000
RAM: DDR5 64GB - ¥30,000
モデル: Qwen3-Coder 7B Q5_K_M

理由:
- コスパ最高
- 実用十分な性能（LiveCodeBench 47.2%）
- 電気代も控えめ
- 2年で元が取れる
```

#### 【プロ向け】ハイエンド構成

```
GPU: RTX 4090 24GB - ¥280,000
CPU: Ryzen 9 7950X3D - ¥80,000
RAM: DDR5 128GB - ¥60,000
モデル: Qwen3-Coder 32B Q4_K_M または DeepSeek V3.1

理由:
- Claude Sonnet 4レベルの性能
- 大規模プロジェクト対応（256K+コンテキスト）
- 将来性あり
```

#### 【コスパ重視】エントリー構成

```
GPU: RTX 4060 8GB - ¥40,000
CPU: Ryzen 5 7600 - ¥30,000
RAM: DDR5 32GB - ¥15,000
モデル: Qwen3-Coder 3B Q4_K_M または Flash 30B-A3B

理由:
- 最小投資
- Flashモデルで高性能（MoE）
- 既存PCのGPU追加でもOK
```

## 10.9 セットアップガイド

### クイックスタート（Ollama + Qwen3-Coder）

**ステップ1**: Ollamaインストール
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**ステップ2**: モデルダウンロード
```bash
ollama pull qwen3-coder:7b-instruct-q4_K_M
```

**ステップ3**: VS Code拡張インストール
- "Continue" または "Cline" をインストール

**ステップ4**: 設定
```json
{
  "models": [{
    "title": "Qwen3-Coder",
    "provider": "ollama",
    "model": "qwen3-coder:7b-instruct-q4_K_M"
  }]
}
```

**ステップ5**: コーディング開始！
- Ctrl+L（Continue）または Clineアイコンクリック
- "この関数をリファクタリングして" などと指示

### トラブルシューティング

**問題**: "Out of Memory" エラー
**解決**: より小さいモデルまたはより強い量子化を使用
```bash
# 7B → 3Bに変更
ollama pull qwen3-coder:3b-instruct-q4_K_M

# またはFlashモデル（MoEで効率的）
ollama pull qwen3-coder:flash-q4_K_M
```

**問題**: 生成が遅い
**解決**: GPU使用を確認、量子化レベルを下げる
```bash
# nvidia-smiでGPU使用確認
nvidia-smi

# Q5 → Q4に変更で高速化
ollama pull qwen3-coder:7b-instruct-q4_K_M
```

**問題**: 品質が低い
**解決**: より大きいモデルまたはより弱い量子化
```bash
# 7B Q4 → 7B Q6に変更
ollama pull qwen3-coder:7b-instruct-q6_K

# または32Bモデルに変更
ollama pull qwen3-coder:32b-instruct-q4_K_M
```

## 10.10 まとめ：最適なローカルLLM選択フローチャート

```
コーディング用ローカルLLM選択
        ↓
   【VRAM容量は？】
        ↓
   ┌────┴────┐
   ↓         ↓
 8GB以下    12GB以上
   ↓         ↓
Qwen3      【用途は？】
Coder 3B      ↓
Q4_K_M   ┌────┴────┐
         ↓         ↓
      補完・生成  推論・デバッグ
         ↓         ↓
   Qwen3-Coder DeepSeek R1-0528
      7B          14B
    Q5_K_M      Q4_K_M

【24GB以上の場合】
    ↓
 Qwen3-Coder 32B Q4_K_M
 または DeepSeek V3.1
 （Claude Sonnet 4レベル）
```

### 最終推奨（2025年10月版）

🥇 **総合1位**: Qwen3-Coder 7B Q5_K_M
- 理由: 性能・VRAM・コストの最適バランス（LiveCodeBench 47.2%）

🥈 **推論特化**: DeepSeek R1-0528 14B Q4_K_M
- 理由: アルゴリズム設計、デバッグに最強（Codeforces ~1930）

🥉 **ハイブリッド**: DeepSeek V3.1
- 理由: 思考モードと高速性の両立（SWE-bench 66.0%）

🎖️ **軽量版**: Qwen3-Coder 3B Q4_K_M または Flash 30B-A3B
- 理由: 低スペックでもMoEで高性能

---

## 関連リンク
- [Ollama公式](https://ollama.com/)
- [LM Studio](https://lmstudio.ai/)
- [Qwen3-Coder - Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-7B-Instruct)
- [Qwen3-Coder - GitHub](https://github.com/QwenLM/Qwen3-Coder)
- [DeepSeek R1-0528](https://github.com/deepseek-ai/DeepSeek-R1)
- [DeepSeek V3.1](https://github.com/deepseek-ai/DeepSeek-V3)
- [Phi-4](https://huggingface.co/microsoft/phi-4)
- [Gemma 3](https://huggingface.co/google/gemma-3-12b)
- [Continue VS Code拡張](https://continue.dev/)
- [Cline VS Code拡張](https://github.com/cline/cline)
