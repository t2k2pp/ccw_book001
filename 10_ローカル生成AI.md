# 第10章: ローカル生成AI - コーディング特化モデルカタログ

プライバシー、コスト、オフライン利用を重視する開発者にとって、ローカル環境で動作するLLMは重要な選択肢です。本章では、コーディング用途に特化したローカルLLMを、マシンスペックと目的に応じて選択できるよう体系的にカタログ化します。

## 10.1 ローカルLLMの概要とメリット

### なぜローカルLLMなのか？

```
クラウドAI                vs              ローカルAI
  ↓                                         ↓
● 高性能                              ● プライバシー保護
● 常に最新                            ● コスト削減（従量課金なし）
● セットアップ不要                    ● オフライン利用可能
● 月額/従量課金                       ● カスタマイズ自由
                                      ● 企業コード流出リスクゼロ
```

### ローカルLLMが最適なケース

✅ **プライバシー重視**
- 企業の機密コードを外部に送信したくない
- セキュリティポリシーでクラウドAI利用が制限される

✅ **コスト削減**
- 大量のコード生成で従量課金が高額になる
- 初期投資後はランニングコストなし

✅ **オフライン環境**
- インターネット接続が不安定
- 機内、移動中でも作業したい

✅ **カスタマイズ**
- 特定のコーディングスタイルに特化させたい
- 社内フレームワーク用にファインチューニング

### 2025年のローカルLLMの進化

2025年のローカルLLMコーディング環境は大きく成熟しました：

- **性能向上**: Qwen3-CoderはClaude Sonnet 4に匹敵
- **小型化**: 1.5B〜7Bモデルでも実用的な性能
- **ツールの充実**: Ollama, LM Studioで簡単セットアップ
- **量子化技術**: 4bit/8bit量子化でVRAM消費を大幅削減

## 10.2 マシンスペック別推奨モデル

### スペック分類とVRAM要件

| スペック | VRAM | 推奨モデルサイズ | 快適度 | 想定ハードウェア |
|---------|------|----------------|--------|----------------|
| **エントリー** | 4-8GB | 1.5B-3B (Q4) | ⭐⭐ | GTX 1660, RTX 3050 |
| **ミドル** | 12-16GB | 7B-8B (Q5-Q6) | ⭐⭐⭐⭐ | RTX 3060, RTX 4060 Ti |
| **ハイエンド** | 24GB | 14B-32B (Q4-Q5) | ⭐⭐⭐⭐⭐ | RTX 3090, RTX 4090 |
| **プロ/ワークステーション** | 40GB+ | 70B+ (Q4) | ⭐⭐⭐⭐⭐ | RTX 4090×2, A100 |

### メモリ計算式

**基本式**:
```
必要VRAM (GB) ≈ モデルパラメータ数 (B) × 量子化ビット数 / 8 × 1.2

例: 7B Q4モデル
= 7 × 4 / 8 × 1.2 = 4.2GB
```

**実用的な目安**:
- 7B Q4: 約4-5GB
- 14B Q4: 約8-9GB
- 32B Q4: 約18-20GB
- 70B Q4: 約42-48GB

### スペック別推奨構成（2025年版）

#### エントリー構成（予算15万円）
```
GPU: RTX 4060 (8GB) - ¥40,000
CPU: Ryzen 5 7600 - ¥30,000
RAM: DDR5 32GB - ¥15,000
SSD: 1TB NVMe - ¥10,000
その他: ¥55,000

推奨モデル: Qwen3-Coder 3B, Phi-4 Mini
```

#### ミドル構成（予算25万円）
```
GPU: RTX 4070 (12GB) - ¥80,000
CPU: Ryzen 7 7700X - ¥45,000
RAM: DDR5 64GB - ¥30,000
SSD: 2TB NVMe - ¥15,000
その他: ¥80,000

推奨モデル: Qwen3-Coder 7B, DeepSeek Coder V2 6.7B
```

#### ハイエンド構成（予算50万円）
```
GPU: RTX 4090 (24GB) - ¥280,000
CPU: Ryzen 9 7950X - ¥80,000
RAM: DDR5 128GB - ¥60,000
SSD: 4TB NVMe - ¥30,000
その他: ¥50,000

推奨モデル: Qwen3-Coder 32B, DeepSeek R1-0528 14B, Llama 3.3 70B (Q4)
```

#### プロ構成（予算100万円+）
```
GPU: RTX 4090 × 2 (48GB total) - ¥560,000
または RTX 5080 32GB × 2 (64GB total)
CPU: Ryzen Threadripper - ¥150,000
RAM: DDR5 256GB - ¥120,000
SSD: 8TB NVMe - ¥60,000
その他: ¥110,000

推奨モデル: Qwen3-Coder 72B, DeepSeek V3.1, Llama 4 70B (Q6)
```

#### 🆕 コンパクトワークステーション構成（AMD Strix Halo）

**MINISFORUM MS-S1 MAX** ¥345,000 (約$2,299)

```
APU: AMD Ryzen AI Max+ 395 (16コアZen 5)
統合GPU: Radeon 8060S (40 RDNA 3.5 CU、最大96GB VRAM利用可)
NPU: XDNA 2 (50 TOPS) + 総合126 TOPS AI性能
RAM: 128GB LPDDR5X-8000 (256GB/s帯域幅)
TDP: 110-160W（4段階調整可能）
電源: 320W内蔵PSU
拡張: PCIe 4.0 x16スロット（x4配線）、デュアルUSB4 V2 (80Gbps)
サイズ: ミニPC（超コンパクト）

特徴:
✅ 70B〜100Bパラメータモデルをローカル実行可能
✅ 統合GPUで96GBまでVRAM利用（業界最大級）
✅ 2台クラスタで235B Q4モデルが10.87 tok/sec
✅ デスクトップ級GPU性能（RTX 4060相当）
✅ 省スペース・低消費電力（160W MAX）

推奨モデル:
- Qwen3-Coder 72B Q4
- Llama 3.3 70B Q4-Q5
- DeepSeek Coder V2 33B Q6
- Gemma 3 27B Q6

評価: ⭐⭐⭐⭐⭐ - コンパクトで超大型LLM実行可能
```

**Strix Haloが最適なケース**:
- 省スペースで高性能が必要
- 70B以上のモデルを動かしたいが、デュアルGPU構成は避けたい
- 電力効率を重視
- 将来的にクラスタ構成を検討

**注意点**:
- 専用GPUと比較してやや低速（RTX 4090比で30-40%低速）
- メモリ帯域がボトルネックになる場合あり
- 日本での入手性は要確認

---

## 10.3 コーディング特化モデルカタログ（2025年版）

### コーディングベンチマーク解説

モデル選択の前に、各ベンチマークが何を測定しているかを理解しましょう：

#### 📊 主要ベンチマーク一覧

| ベンチマーク | 測定内容 | 問題数 | 特徴 | 重視する人 |
|------------|----------|---------|------|-----------|
| **HumanEval** | 基本的なPythonコーディング | 164問 | アルゴリズム・数学、面接問題レベル | 初学者、Python開発者 |
| **MBPP** | 基礎プログラミング | 974問 | エントリーレベル、自然言語→コード | 教育・学習用途 |
| **LiveCodeBench** | 実践的コーディング | 定期更新 | 最新の実務問題、汚染防止 | 実務開発者 |
| **SWE-bench** | 実際のGitHub Issue修正 | 2,294問 | 実プロジェクトのバグ修正 | エンジニアチーム |
| **Codeforces** | 競技プログラミング | - | アルゴリズム設計、最適化 | アルゴ得意者、研究者 |
| **AIME** | 数学・論理推論 | 30問 | 高度な数理問題 | 数学・科学計算 |

#### 💡 ベンチマークの読み解き方

**HumanEval 80%以上**
→ 基本的なコーディングタスクは問題なくこなせる

**LiveCodeBench 60%以上**
→ 実務レベルの複雑なコーディングが可能

**SWE-bench 60%以上**
→ 実際のバグ修正・リファクタリングが得意

**Codeforces 1800+**
→ アルゴリズム設計・最適化に強い（競技プログラミング青〜黄色レベル）

**AIME 90%以上**
→ 高度な数学的推論が可能

### 統一ベンチマーク性能比較（2025年10月時点）

同じ基準で比較できるよう、主要ベンチマークを横並びで表示します。空欄（-）は公開データなし。

| モデル | 開発元 | HumanEval | MBPP | LiveCodeBench | SWE-bench | Codeforces | AIME | 総合評価 |
|--------|--------|-----------|------|---------------|-----------|------------|------|---------|
| **Qwen3-Coder** | Alibaba | 88.3% (7B) | - | 70.6% (235B) | 69.6% | 2,056 (235B) | - | ⭐⭐⭐⭐⭐ |
| **DeepSeek R1-0528** | DeepSeek | ~90% | - | - | ~49% | ~1,930 (70B) | - | ⭐⭐⭐⭐⭐ |
| **GLM-4.6** 🆕 | Zhipu AI | 84.2%* | 80.7%* | 82.8% (v6) | 68.0% | - | **98.6%** | ⭐⭐⭐⭐⭐ |
| **DeepSeek V3.1** | DeepSeek | 82.6% | - | - | 66.0% | - | - | ⭐⭐⭐⭐⭐ |
| **Phi-4** | Microsoft | 82.6% (14B) | - | - | - | - | - | ⭐⭐⭐⭐⭐ |
| **OpenAI GPT-OSS** 🆕 | OpenAI | 62% (20B) | - | - | - | - | - | ⭐⭐⭐⭐ |
| **Qwen3-VL** | Alibaba | 画像→コード特化 | - | - | - | - | - | ⭐⭐⭐⭐⭐ |
| **Kimi K2** | Moonshot AI | - | - | - | - | - | 最先端 | ⭐⭐⭐⭐⭐ |

*非公式値（中国語技術ブログより）

#### 📌 参考：プロプライエタリモデル（比較用）

| モデル | HumanEval | SWE-bench | 特徴 |
|--------|-----------|-----------|------|
| Claude Sonnet 4.5 | 92% | 77.2% | メンター型、説明最強 |
| GPT-4o | 90.2% | - | バランス型 |
| Gemini 2.5 Pro | - | 63.8% | UI/Web開発最強、100万トークン |

### クイック選択ガイド

**HumanEval最強**: DeepSeek R1 (~90%) → Qwen3-Coder (88.3%)
**LiveCodeBench最強**: GLM-4.6 (82.8%) → Qwen3-Coder (70.6%)
**SWE-bench最強**: Qwen3-Coder (69.6%) → GLM-4.6 (68.0%)
**数学推論最強**: GLM-4.6 (AIME 98.6%) → Kimi K2
**競技プログラミング最強**: Qwen3-Coder (Codeforces 2,056)

### モデルの使用感・特性比較

実際の使用感とコーディングスタイルの違いを理解すると、自分に合ったモデルを選びやすくなります。

#### 🎨 コーディングスタイル別分類

| モデル | スタイル | 似ているモデル | 特徴 | 向いている人 |
|--------|---------|--------------|------|------------|
| **Qwen3-Coder** | 効率重視・クリーン | Claude的（軽量版） | クリーンなコード、高速、説明は簡潔 | コスパ重視、実務開発者 |
| **DeepSeek R1** | 推論重視・論理派 | o1的 | 段階的推論、論理的一貫性、長い応答時間（~8分） | アルゴリズム設計、デバッグ |
| **DeepSeek V3.1** | ハイブリッド型 | Claude + o1 | 思考モード切替可能、自然な説明、シンプル | 汎用性重視、状況に応じて使い分け |
| **GLM-4.6** | コード中心・実用 | ChatGPT的 | 機能優先、説明少なめ、ツール呼び出し最強 | エージェント開発、API統合 |
| **Phi-4** | 数学特化 | - | STEM推論強い、Python中心、実用性は限定的 | 数学・科学計算、教育 |
| **OpenAI GPT-OSS** | バランス型 | GPT-3.5的 | 中堅性能、MIT License、商用自由 | ライセンス重視、カスタマイズ |

#### 💬 説明スタイルの違い

**メンター型（説明豊富）**
- 🥇 **Claude Sonnet 4.5**（プロプライエタリ）: 最も丁寧、安全性配慮
- 🥈 DeepSeek V3.1: 自然な説明、中間サマリーあり
- 🥉 Qwen3-Coder: 必要十分な説明、簡潔

**コード中心型（説明少なめ）**
- 🥇 GLM-4.6: 機能優先、コードで語る
- 🥈 Phi-4: 数式・コード中心
- 🥉 OpenAI GPT-OSS: シンプルな出力

#### 🚀 レスポンス速度

| 速度 | モデル | 備考 |
|------|--------|------|
| ⚡⚡⚡ | Qwen3-Coder | 最速クラス |
| ⚡⚡⚡ | DeepSeek V3.1 | V3モード時 |
| ⚡⚡ | GLM-4.6 | 標準的 |
| ⚡⚡ | Phi-4 | 標準的 |
| ⚡ | DeepSeek R1 | 推論モード時は遅い（~8分） |
| ⚡⚡ | OpenAI GPT-OSS | 標準的 |

#### 🎯 使用場面別おすすめ

**普段Claude使っている人には**
→ **Qwen3-Coder** or **DeepSeek V3.1**
理由: Claudeに近い使用感、高品質な出力、説明も充実

**普段ChatGPT使っている人には**
→ **GLM-4.6** or **OpenAI GPT-OSS**
理由: 実用的な出力スタイル、ツール連携強い

**普段Gemini使っている人には**
→ **Qwen3-Coder** or **Qwen3-VL**（UI開発時）
理由: マルチモーダル対応、長文コンテキスト

**競技プログラミング経験者には**
→ **DeepSeek R1** or **Qwen3-Coder**
理由: アルゴリズム最適化、Codeforces高スコア

**数学・科学計算メインには**
→ **GLM-4.6** or **Phi-4**
理由: AIME高スコア、数理推論特化

**エージェント開発には**
→ **GLM-4.6**
理由: ツール呼び出し成功率90.6%（業界最高）

**UI/Web開発には**
→ **Qwen3-VL** or **DeepSeek V3.1**
理由: ビジュアルコーディング、フロントエンド生成

#### ⚠️ 注意点

**DeepSeek R1**: 複雑な問題では推論時間が非常に長い（最大8分）ので、リアルタイム補完には不向き

**Phi-4**: Pythonと一部パッケージに偏った学習データのため、実用性は限定的

**GLM-4.6**: 推論モード有効時はツール呼び出しが無効になるため、エージェント用途では推論モードを切る必要あり

**OpenAI GPT-OSS**: 中堅性能のため、最先端タスクには力不足の可能性

### 詳細モデルカタログ

---

#### 🏆 Qwen3-Coder（最推奨）

**開発元**: Alibaba Qwen Team
**リリース**: 2025年7月22日
**ライセンス**: Apache 2.0

**主要モデル**:
| モデル | Total Params | Active Params | VRAM (Q4) | 用途 |
|--------|--------------|---------------|-----------|------|
| **Qwen3-Coder-480B-A35B** | 480B | 35B | 90GB | フラッグシップ・大規模用途 |
| **Qwen3-Coder-Flash (30B-A3B)** | 30.5B | 3.3B | 8GB | 高速・実用バランス |
| **Qwen3-32B** | 32B | 32B | 20GB | 密モデル・高性能 |
| **Qwen3-14B** | 14B | 14B | 9GB | ミドル〜ハイエンド |
| **Qwen3-7B** | 7B | 7B | 4.5GB | 標準・バランス |
| **Qwen3-3B** | 3B | 3B | 2.5GB | 軽量・エントリー |

**革新的な特徴**:
- **MoE (Mixture-of-Experts)**: 160 experts、8 activated per inference
- **超大規模コンテキスト**: 256K tokens native、1M tokens拡張可能
- **7.5T トークン学習**: コード比率70%の大規模データセット
- **LiveCodeBench 47.2**: Qwen 2.5の38.7から22%向上
- **エージェント型コーディング**: Claude Sonnet級の性能

**ベンチマーク**:
- HumanEval: 88.3% (7B)
- LiveCodeBench: 47.2% (235B)
- Aider-Edit: 84.2%+
- 100+プログラミング言語対応

**推奨量子化**:
- 8GB VRAM: 3B-Q4 または Flash 30B-A3B Q4
- 12GB VRAM: 7B-Q5 または 14B-Q4
- 24GB VRAM: 32B-Q4
- 48GB+ VRAM: 480B-A35B Q4

**Ollamaでの使用**:
```bash
# Qwen3-Coder 7B (推奨)
ollama pull qwen3-coder:7b-instruct-q4_K_M

# Qwen3-Coder 32B (高性能)
ollama run qwen3-coder:32b-instruct-q4_K_M
```

**Qwen Code CLI最新機能**（2025年10月）🆕:

Alibaba CloudがQwen Code CLIをアップデート（v0.0.12 → v0.0.14）し、2つの革新的機能を追加：

1. **Plan Mode**（プランモード）
   - AI生成コードを実行前にレビュー・承認
   - 意図しないコード上書きを防止
   - 信頼性の高い開発ワークフロー実現

2. **Vision Intelligence**（ビジョンインテリジェンス）
   - 画像入力を自動認識し、Qwen3-VL-Plusモデルに切り替え
   - UI設計図、アーキテクチャ図、エラースクリーンショットを処理
   - テキストとビジュアルのシームレスな統合

3. **Deep Research Update**（2025年10月21日）
   - 研究レポート→ライブウェブページ＋ポッドキャストを自動生成
   - Qwen3-Coder、Qwen-Image、Qwen3-TTSを統合活用
   - マルチフォーマット出力で効率的なコンテンツ制作

**CLI使用例**:
```bash
# インストール
pip install qwen-code-cli

# Plan Modeで安全にコーディング
qwen-code --plan-mode "新機能を追加"

# Vision Intelligenceで画像解析
qwen-code "この設計図をReactで実装" --image design.png
```

**評価**: ⭐⭐⭐⭐⭐ - 2025年の最強コーディングLLM（Claude Sonnet 4に匹敵）

---

#### 🖼️ Qwen3-VL（ビジュアルコーディング特化）

**開発元**: Alibaba Qwen Team
**リリース**: 2025年10月14日
**ライセンス**: Apache 2.0

**主要モデル（8Bシリーズ）**:
| モデル | パラメータ | VRAM (Q4) | 特徴 | 用途 |
|--------|----------|-----------|------|------|
| **Qwen3-VL-8B-Instruct** | 8.77B | 6GB | 高速・汎用 | 一般的なビジュアルコーディング |
| **Qwen3-VL-8B-Thinking** | 8.77B | 6GB | 推論重視 | STEM問題・複雑な視覚分析 |

**その他のサイズ**:
- **Qwen3-VL-4B** (Instruct / Thinking): 超軽量版
- **Qwen3-VL-32B**: 高性能密モデル
- **Qwen3-VL-30B**: MoE版
- **Qwen3-VL-235B**: 最大性能MoE版（GPT-5 Mini級）

**革新的な特徴**:
- **ビジュアルコーディング**: 画像・スクリーンショットからHTML/CSS/JavaScript生成
- **256K→1Mコンテキスト**: 動画全体の理解と分析
- **OCRBench 896点**: テキスト認識で業界トップクラス
- **GUI/エージェント制御**: PC/モバイルGUI操作が可能
- **32言語OCR**: 多言語テキスト認識

**ベンチマーク**:
- OCRBench: 896点（8B）- Gemini 2.5 Flash Lite超え
- ビジュアルコーディング: 画像→コード変換で高精度
- STEM推論: Thinkingモデルで特に強力
- 処理速度: Instruct版はThinking版の1.5-2倍高速

**Instruct vs Thinking**:

| 項目 | Qwen3-VL-8B-Instruct | Qwen3-VL-8B-Thinking |
|------|---------------------|---------------------|
| **最適化** | 高速性・汎用性 | 推論深度・精度 |
| **処理時間** | 1.5-2x高速 | 詳細な思考プロセス |
| **出力量** | 標準 | 4倍の詳細出力 |
| **用途** | チャット、迅速な応答 | STEM、複雑な因果分析 |

**推奨用途**:
- **Instruct**: 画像からのコード生成、迅速なUI設計、スクリーンショット分析
- **Thinking**: 図解の詳細解析、STEM問題の視覚的解説、複雑なGUI理解

**Ollamaでの使用**:
```bash
# Qwen3-VL-8B-Instruct (推奨・高速)
ollama pull qwen3-vl:8b-instruct-q4_K_M
ollama run qwen3-vl:8b-instruct-q4_K_M

# Qwen3-VL-8B-Thinking (推論重視)
ollama pull qwen3-vl:8b-thinking-q4_K_M

# クラウド版（235B）
ollama run qwen3-vl:235b-cloud
```

**ビジュアルコーディング実例**:
```bash
# 画像ファイルをコードに変換
ollama run qwen3-vl:8b-instruct "この画像のUIをReactコンポーネントに変換して" < design.png

# スクリーンショットから再現
ollama run qwen3-vl:8b-instruct "このウェブページのHTMLとCSSを生成して" < screenshot.png
```

**FP8量子化**:
- Qwen3-VL-8B-Instruct-FP8: BF16とほぼ同等性能（1%以内）
- VRAM削減: 約40%軽量化
- ブロックサイズ128のfine-grained量子化

**評価**: ⭐⭐⭐⭐⭐ - ビジュアルコーディング最強（画像→コード変換）

---

#### 🔥 DeepSeek R1-0528（推論特化）

**開発元**: DeepSeek
**リリース**: 2025年5月28日
**ライセンス**: MIT

**特徴**:
- **Chain-of-Thought推論**: 推論プロセスを可視化
- **Codeforces ~1930**: 従来の~1530から+400ポイント向上
- 数学・論理的思考に強い
- 日本語版あり（Lightblue/サイバーエージェント）

**モデルバリエーション**:
| サイズ | VRAM (Q4) | Codeforces | 特徴 |
|--------|----------|-----------|------|
| 1.5B | 1.5GB | - | 軽量版 |
| 7B | 4.5GB | - | 標準版 |
| 14B | 9GB | ~1930 | ベストバランス |
| 70B | 45GB | - | シリーズ最大 |

**強み**:
- 複雑なアルゴリズム設計（Codeforces競技プログラミングレベル）
- デバッグ支援（思考過程をステップバイステップ表示）
- アーキテクチャ設計の提案
- O3、Gemini 2.5 Proに接近する性能

**ベンチマーク（R1-0528）**:
- Codeforces: ~1930 (従来比+400ポイント)
- 数学、コーディング、推論で大幅向上

**Ollamaでの使用**:
```bash
# DeepSeek R1-0528 14B (推奨)
ollama pull deepseek-r1:14b-qwen-distill-q4_K_M

# 日本語特化版
ollama pull lightblue/deepseek-r1-distill-qwen-14b-japanese
```

**評価**: ⭐⭐⭐⭐⭐ - 推論タスク最強（2025年5月アップデート版）

---

#### 🌊 DeepSeek V3.1（ハイブリッド思考モデル）

**開発元**: DeepSeek
**リリース**: 2025年8月19日
**ライセンス**: MIT

**特徴**:
- **ハイブリッドアーキテクチャ**: V3とR1を統合した単一モデル
- **思考モード切替**: チャットテンプレートで「思考モード」と「非思考モード」を切り替え可能
- **671B Total / 37B Active**: MoEアーキテクチャで高効率

**モデルスペック**:
| パラメータ | VRAM (Q4) | コンテキスト | 特徴 |
|----------|----------|-------------|------|
| 671B (37B active) | 約100GB | 128K tokens | 思考モード搭載 |

**強み**:
- **SWE-bench Verified 66.0%**: DeepSeek R1の44.6%を大幅超え
- **Aider Polyglot 71.6%**: Claude 4 Opusを上回る
- **プログラミング特化**: マルチ言語コード生成で業界トップクラス
- DeepSeek R1の推論能力 + V3の高速性を同時に実現

**ベンチマーク**:
- SWE-bench Verified: 66.0% (R1-0528: 44.6%)
- Aider Polyglot: 71.6%
- BF16、FP8、F32対応

**評価**: ⭐⭐⭐⭐⭐ - 思考と速度の両立（2025年8月の最新ハイブリッドモデル）

---

#### 🚀 DeepSeek Coder V2

**開発元**: DeepSeek
**リリース**: 2024年

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 特徴 |
|--------|----------|------|
| 6.7B | 4.2GB | コンパクト高性能 |
| 33B | 20GB | ハイエンド |
| 236B MoE | 90GB | 最大性能版 |

**強み**:
- Fill-in-the-Middle対応（カーソル位置でのコード補完）
- **338プログラミング言語対応**（業界最多クラス）
- レポジトリレベルのコード理解
- 236B MoEバージョンで超大規模コードベース対応

**システム要件**:
- 6.7B Lite: 40GB VRAM (BF16) / 10.4GB (Q4)
- 236B MoE: 推奨90GB+ VRAM

**評価**: ⭐⭐⭐⭐⭐ - コード補完に特化

---

#### 🦙 Llama 4（Meta最新・マルチモーダルMoE）

**開発元**: Meta
**リリース**: 2025年4月5日
**ライセンス**: Llama 4 License

**モデルバリエーション**:
| モデル | パラメータ | 特徴 | ステータス |
|--------|----------|------|-----------|
| **Llama 4 Scout** | 小型MoE | 軽量・高速 | ✅ リリース済み |
| **Llama 4 Maverick** | 中型MoE | バランス型 | ✅ リリース済み |
| **Llama 4 Behemoth** | 超大型MoE | シリーズ最大 | 🚧 トレーニング中 |

**革新的な特徴**:
- **Mixture-of-Experts (MoE)**: Llamaシリーズ初のMoEアーキテクチャ
- **マルチモーダル**: テキスト、画像、動画を統合処理
- **12言語対応**: 多言語コーディング支援
- 音声・推論機能の大幅強化

**強み**:
- 汎用LLMとしての高い性能（コーディング以外も強い）
- マルチモーダルでUI/UX設計にも活用可能
- Metaの最先端技術を凝縮

**注意**:
- コーディング特化ではQwen3-Coderが上位
- Behemothは未リリース（2025年10月時点）
- CodeLlamaシリーズは事実上、Llama 4に統合

**評価**: ⭐⭐⭐⭐ - マルチモーダル対応の汎用LLM（コーディング特化ではない）

---

#### 🔵 Phi-4（最新 - コーディング特化）

**開発元**: Microsoft
**リリース**: 2025年1月
**ライセンス**: MIT

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 特徴 |
|--------|----------|------|
| Phi-4-Mini (3.8B) | 2.5GB | 軽量版 |
| Phi-4 (14B) | 9GB | 標準版 |
| Phi-4-reasoning (14B) | 9GB | 推論特化 |

**強み**:
- **HumanEval 82.6**: LLaMA 3.3 70Bを上回る驚異的なスコア
- **LiveCodeBench 53.8%**: コーディング性能が大幅向上（+25ポイント）
- **MATH 80.4**: 数学的推論も優秀
- 14Bで70Bクラスの性能を実現

**ベンチマーク**:
- HumanEval: 82.6
- LiveCodeBench: 53.8%
- MATH: 80.4
- GSM-8K: 88.6% (Mini版)

**推奨用途**:
- 中スペックGPU（12-16GB）でのハイパフォーマンス
- 数学・論理的コーディングタスク
- コンパクトながら高品質な生成が必要な場合

**Ollamaでの使用**:
```bash
# Phi-4 14B (推奨)
ollama pull phi4:14b-q4_K_M

# Phi-4 Mini 3.8B (軽量版)
ollama pull phi4:mini-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - 14Bで最高クラスのコーディング性能

---

#### 🔓 OpenAI GPT-OSS（OpenAI初のオープンウェイト）🆕

**開発元**: OpenAI
**リリース**: 2025年8月
**ライセンス**: MIT License（完全オープンウェイト）

**モデルバリエーション**:
| モデル | パラメータ | VRAM (Q4) | 特徴 |
|--------|----------|-----------|------|
| **GPT-OSS-20B** | 20B | 12GB | 小型・実用モデル |
| **GPT-OSS-120B** | 120B | 70GB | 大型・高性能モデル |

**歴史的意義**:
- **OpenAI初のオープンウェイトモデル**: GPT-2（2019年）以来、6年ぶり
- **完全MIT License**: 商用利用・再配布・改変すべて可能
- **エコシステム対応**: Ollama、vLLM、llama.cpp、LM Studio、Apple Metal対応

**ベンチマーク**:
- **コード生成**: 62% pass@1（HumanEval相当）
- **競合比較**: クローズドソースモデルと遜色ない性能
- **汎用性能**: 数学、推論、コーディングでバランス良好

**技術仕様**:
- **コンテキスト**: 8K-32K（モデルにより異なる）
- **量子化対応**: Q4, Q5, Q8, FP16すべて利用可能
- **推論速度**: Ollama最適化で高速動作

**強み**:
- **OpenAIブランド**: 信頼性と品質保証
- **広範な対応**: 主要なローカルLLMツールすべてで動作
- **コミュニティサポート**: 迅速なバグ修正と改善
- **企業利用**: MIT Licenseで安心して導入可能

**推奨用途**:
- OpenAIエコシステムをローカル環境で再現
- 企業でのプロトタイピング・開発
- 教育・研究目的
- プライバシー重視のアプリケーション

**Ollamaでの使用**:
```bash
# GPT-OSS-20B (推奨・実用バランス)
ollama pull gpt-oss:20b-q4_K_M
ollama run gpt-oss:20b-q4_K_M

# GPT-OSS-120B (高性能)
ollama pull gpt-oss:120b-q4_K_M
```

**注意点**:
- コーディング特化ではない（汎用LLM）
- Qwen3-Coder、DeepSeek専門モデルには劣る
- しかしOpenAIブランドの信頼性は大きな価値

**評価**: ⭐⭐⭐⭐⭐ - OpenAI初のオープンウェイト、歴史的リリース

---

#### 🎯 GLM-4シリーズ（Zhipu AI - ツール呼び出し最強）

**開発元**: Zhipu AI（清華大学）
**リリース**: 2024年〜2025年
**ライセンス**: GLM-4 License（商用可）

**主要モデル**:
| モデル | パラメータ | VRAM (Q4) | 特徴 |
|--------|----------|-----------|------|
| **GLM-4-9B-Chat** | 9B | 6GB | 基本版、Ollama対応 |
| **GLM-4.5** | 非公開 | API専用 | SWE-bench 64.2% |
| **GLM-4.6** 🆕 | 357B | API/ローカル | 最新版（2025年9月）、200Kコンテキスト |
| **GLM-Z1-9B** | 9B | 6GB | 数学推論特化 |
| **GLM-4.1V-9B-Thinking** | 9B | 6GB | 推論可視化版 |

**革新的な特徴**:
- **ツール呼び出し成功率90.6%**: 業界最高（Qwen3、Kimi K2を上回る）
- **200Kコンテキスト** 🆕: GLM-4.6（128K→200K拡張）
- **26言語対応**: 日本語、韓国語、ドイツ語など
- **RAGタスクで幻覚率1.3%**: 業界トップクラスの正確性
- **MIT License** 🆕: GLM-4.6はオープンウェイト

**ベンチマーク**:
- HumanEval: 70.1% (9B) - Llama-3-8Bより+7.9%
- SWE-bench Verified: 64.2% (GLM-4.5) - GPT-4レベル
- **AIME 2025** 🆕: 93.9%（標準）、**98.6%（ツール使用時）** - 高校数学コンテスト
- **CC-Bench** 🆕: Claude Sonnet 4に対して48.6%勝率
- ツール呼び出し: 90.6% (GLM-4.5) - 業界最高
- エージェント型タスク: Qwen3-Coderに対して80.8%勝率

**GLM-4.6の特筆すべき性能** 🆕:
- **357Bパラメータ**: 大規模MoEアーキテクチャ
- **200K→エージェント対応**: 超長文＋マルチステップタスク
- **フロントエンド開発**: CC-Benchで実践的コーディングに強い
- **コスト効率**: 性能対コスト比でClaude Sonnet 4に迫る

**強み**:
- **エージェント型ワークフロー**: API統合、データベース連携に最強
- **Function Calling**: カスタムツール呼び出しが業界最高精度
- **マルチモーダル**: GLM-4.1V-9Bシリーズで画像・テキスト対応
- **低幻覚率**: RAGアプリケーションで実用性が高い

**推奨用途**:
- ツール統合が必要なエージェント型開発
- 外部API連携の自動化
- RAGアプリケーション（幻覚率最小化）
- マルチターン会話での複雑なタスク

**Ollamaでの使用**:
```bash
# GLM-4-9B-Chat (推奨)
ollama pull glm4:9b-chat-q4_K_M

# GLM-Z1-9B (数学推論特化)
ollama pull glm-z1:9b-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - ツール呼び出し・エージェント型タスク最強

---

#### 🌙 Kimi K2（Moonshot AI - 超長文対応）

**開発元**: Moonshot AI（月之暗面）
**リリース**: 2025年7月（最新版: K2-0905、2025年9月）
**ライセンス**: Modified MIT License（商用可）

**モデルスペック**:
| 項目 | 詳細 |
|------|------|
| **Total Params** | 1 Trillion (1T) |
| **Active Params** | 32B（MoE） |
| **コンテキスト** | 256K tokens（2025年9月版で128K→256K） |
| **VRAM (推奨)** | 80GB+（Q4量子化で60GB程度） |

**革新的な特徴**:
- **1兆パラメータ級**: 32B activeで驚異的な性能
- **256Kコンテキスト**: 業界トップクラスの長文対応
- **コスト効率**: API利用でコスパに優れる
- **ローカルデプロイ対応**: Ollama、vLLM、SGLang、TensorRT-LLM

**ベンチマーク**:
- 数学・コーディングで最先端性能
- 非推論モデルの中でフロンティア知識、数学、コーディングでSOTA
- ツール呼び出し・エージェント機能も強力

**強み**:
- **超長文処理**: 256Kコンテキストで大規模コードベース分析
- **レポジトリ規模の理解**: プロジェクト全体を一度に処理可能
- **フロントエンド開発**: K2-0905でフロントエンド性能が大幅向上
- **プライバシー重視**: ローカルデプロイで機密コード保護

**推奨用途**:
- 大規模コードベースの全体分析
- マイクロサービスアーキテクチャのリファクタリング
- 長文ドキュメント解析
- データプライバシーが重要なエンタープライズ開発

**デプロイ方法**:
```bash
# Ollama（対応予定、または手動GGUF変換）
# Hugging Faceからblock-fp8形式でダウンロード可能

# vLLM / SGLang推奨
# 公式のModel Deployment Guideを参照
```

**評価**: ⭐⭐⭐⭐⭐ - 超長文対応とコスト効率に優れる

---

#### ⚡ Phi-3 Mini（旧世代）

**開発元**: Microsoft
**サイズ**: 3.8B
**VRAM**: 2.5GB (Q4)

**強み**:
- 非常に軽量
- GPUなしでもCPUで動作可能
- モバイルデバイス対応

**用途**:
- 低スペックPC
- ラップトップでの軽量補完
- 学習用

**注意**: Phi-4 Miniの登場により、新規導入は推奨しない。

**評価**: ⭐⭐⭐ - 軽量だが性能は控えめ（Phi-4への移行推奨）

---

#### 🔷 Gemma 3（Google最新・マルチモーダル）

**開発元**: Google / DeepMind
**リリース**: 2025年3月
**ライセンス**: Gemma License（商用可）
**ベース技術**: Gemini 2.0と同じ研究・技術

**モデルバリエーション**:
| サイズ | VRAM (Q4) | マルチモーダル | コンテキスト | 特徴 |
|--------|----------|--------------|-------------|------|
| 270M | 0.5GB | テキストのみ | - | 超軽量ファインチューニング用 |
| 1B | 1GB | テキストのみ | 128K | モバイル向け |
| 4B | 3GB | 画像+テキスト | 128K | バランス型 |
| 12B | 8GB | 画像+テキスト | 128K | 高性能 |
| 27B | 17GB | 画像+テキスト | 128K | シリーズ最大 |

**強み**:
- **マルチモーダル**: 画像とテキストを同時処理（4B以上）
- **大規模コンテキスト**: 128K tokens（約300ページの本、30枚の高解像度画像、1時間以上の動画）
- **Gemini 2.0ベース**: Googleの最先端技術を凝縮
- コーディング、数学、推論で優れた性能
- 140+言語対応

**ベンチマーク**:
- HumanEval・MBPPで高スコア
- 27BはA100 80GB・H100で効率的に動作
- 世界最高のシングルアクセラレータモデル（27B）

**推奨用途**:
- マルチモーダルコーディング（コード + 図解）
- 多言語コード生成
- 数学・科学系コーディング
- モバイル/エッジデバイス（1B、270M）

**Ollamaでの使用**:
```bash
# Gemma 3 12B (推奨バランス型)
ollama pull gemma3:12b-instruct-q4_K_M

# Gemma 3 27B (高性能)
ollama pull gemma3:27b-instruct-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - Gemini 2.0技術のオープン版

---

#### 📱 Gemma 3n（モバイル特化・効率重視）

**開発元**: Google / DeepMind
**リリース**: 2025年6月
**ライセンス**: Gemma License（商用可）
**特徴**: Gemini Nanoと同じアーキテクチャ

**モデルバリエーション**:
| モデル | 実際のパラメータ | 実効メモリ | マルチモーダル |
|--------|----------------|-----------|--------------|
| E2B | 5B | 2GB | 音声+画像+テキスト+動画 |
| E4B | 8B | 3GB | 音声+画像+テキスト+動画 |

**革新的アーキテクチャ**:
- **Matryoshka Transformer (MatFormer)**: 大きなモデル内に小さなモデルをネスト
- **Per-Layer Embedding (PLE)**: レイヤーごとの埋め込みでパフォーマンス向上
- **MobileNet-V5-300M**: 新世代ビジョンエンコーダー
- **音声処理**: 30秒までの音声クリップを処理（転写・翻訳）

**強み**:
- 5Bパラメータが2GBメモリで動作（従来の2Bモデル相当）
- モバイル・エッジデバイスで最高クラスの性能
- 音声、画像、動画、テキストを統合処理

**推奨用途**:
- モバイルアプリ開発
- エッジAIコーディング支援
- 低メモリ環境での高性能推論
- マルチモーダルタスク

**評価**: ⭐⭐⭐⭐ - モバイル/エッジ最強モデル

---

#### 🌟 StarCoder2

**開発元**: BigCode
**サイズ**: 3B, 7B, 15B

**強み**:
- オープンソースコードで学習
- 透明性が高い
- コミュニティ主導

**評価**: ⭐⭐⭐⭐

---

#### 🦙 Llama 3.3

**開発元**: Meta
**サイズ**: 70B
**VRAM**: 45GB (Q4)

**特徴**:
- 汎用LLMとしての性能
- コーディング以外も強い
- 推論能力向上

**評価**: ⭐⭐⭐⭐ - 汎用性は高いがコーディング特化には劣る

---

## 10.4 用途別モデル選択ガイド

### シーン1: 日常的なコード補完

**目的**: VS Codeでリアルタイム補完

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| エントリー | Qwen3-Coder 3B Q4 | 軽量で高速 |
| ミドル | Qwen3-Coder 7B Q5 | ベストバランス |
| ハイエンド | Qwen3-Coder 14B Q4 | 最高品質 |

---

### シーン2: 複雑なアルゴリズム設計

**目的**: データ構造やアルゴリズムの実装

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | DeepSeek R1-0528 7B | 推論過程が見える |
| ハイエンド | DeepSeek R1-0528 14B | 最適 |
| プロ | Qwen3-Coder 32B | 最高性能 |

---

### シーン3: リファクタリング

**目的**: 既存コードの改善

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | Qwen3-Coder 7B Q6 | コードベース理解 |
| ハイエンド | Qwen3-Coder 32B Q4 | 大規模リファクタリング |

---

### シーン4: デバッグ支援

**目的**: バグの原因特定と修正

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | DeepSeek R1-0528 7B | 思考過程でバグ特定 |
| ハイエンド | DeepSeek R1-0528 14B | 複雑なバグ対応 |

---

### シーン5: プロジェクト全体の理解

**目的**: 大規模コードベースの分析

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ハイエンド | Qwen3-Coder 32B | 256Kコンテキスト |
| プロ | Qwen3-Coder 480B-A35B または Kimi K2 | 最大理解力・100万トークン対応 |

---

### シーン6: エージェント型開発・ツール統合

**目的**: 外部API連携、データベース操作、マルチツール統合

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | GLM-4-9B-Chat | ツール呼び出し90.6% |
| ハイエンド | GLM-4.5 / 4.6 (API) | 業界最高精度 |

**GLM-4の優位性**:
- Function Calling精度90.6%（業界トップ）
- API統合・データベース連携に特化
- RAGアプリで幻覚率1.3%

---

### シーン7: 超大規模コードベース（100K+ tokens）

**目的**: マイクロサービス全体、モノレポ分析

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| プロ（80GB+ VRAM） | Kimi K2 | 256Kコンテキスト、1Tパラメータ |
| プロ（48GB+ VRAM） | Qwen3-Coder 480B-A35B | 100万トークン対応 |

**Kimi K2の優位性**:
- 256Kネイティブコンテキスト
- レポジトリ全体を一度に処理
- フロントエンド開発に強い（K2-0905）

---

### シーン8: ビジュアルコーディング・UI/UX開発

**目的**: デザインモックアップからのコード生成、スクリーンショット再現

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル（8GB VRAM） | Qwen3-VL-8B-Instruct | 高速・実用的 |
| ハイエンド | Qwen3-VL-32B | 高精度 |
| クラウド | Qwen3-VL-235B | 最高性能 |

**Qwen3-VLの優位性**:
- 画像→HTML/CSS/JavaScript自動生成
- OCRBench 896点（テキスト認識）
- デザインツール（Figma、Sketch）のスクリーンショット対応
- レスポンシブデザイン生成

**具体的なユースケース**:
1. **デザインモックアップ → コード**: Figma画像をReactコンポーネント化
2. **既存サイト再現**: スクリーンショットから同等のHTML/CSS生成
3. **UI改善提案**: 現在のUIを読み取り、改善コードを提案
4. **手書きワイヤーフレーム**: 紙のスケッチをコード化

**推奨ワークフロー**:
```bash
# 1. デザイン画像を用意
# 2. Qwen3-VLで変換
ollama run qwen3-vl:8b-instruct "この画像のレイアウトをTailwind CSSで再現して" < design.png

# 3. 生成されたコードを確認・調整
# 4. 反復改善
```

---

## 10.5 ローカルLLM実行ツール

### Ollama vs LM Studio 比較

| 特徴 | Ollama | LM Studio |
|------|--------|-----------|
| **インターフェース** | CLI + 簡易GUI | フルGUI |
| **使いやすさ** | 開発者向け | 初心者向け |
| **柔軟性** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| **API提供** | REST API標準 | あり |
| **対応OS** | Mac, Linux, Windows | Mac, Windows |
| **モデル管理** | CLI | GUI（ドラッグ&ドロップ） |
| **推奨用途** | 開発統合、自動化 | 手軽に試す、チャット |

### Ollama セットアップ（推奨）

#### インストール

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# https://ollama.com/download からインストーラーをダウンロード
```

#### モデルのダウンロードと実行

```bash
# Qwen3-Coder 7B (推奨)
ollama pull qwen3-coder:7b-instruct-q4_K_M
ollama run qwen3-coder:7b-instruct-q4_K_M

# Qwen3-Coder 32B (高性能)
ollama pull qwen3-coder:32b-instruct-q4_K_M

# DeepSeek R1-0528 14B
ollama pull deepseek-r1:14b-qwen-distill-q4_K_M
ollama run deepseek-r1:14b-qwen-distill-q4_K_M

# Phi-4 14B
ollama pull phi4:14b-q4_K_M

# GLM-4-9B-Chat (ツール統合向け)
ollama pull glm4:9b-chat-q4_K_M
ollama run glm4:9b-chat-q4_K_M

# Qwen3-VL-8B-Instruct (ビジュアルコーディング)
ollama pull qwen3-vl:8b-instruct-q4_K_M
ollama run qwen3-vl:8b-instruct-q4_K_M
```

#### VS Code統合（Continue拡張）

```bash
# Continue拡張をインストール
# VS Code > Extensions > "Continue" で検索

# 設定例 (~/.continue/config.json)
{
  "models": [
    {
      "title": "Qwen3-Coder",
      "provider": "ollama",
      "model": "qwen3-coder:7b-instruct-q4_K_M"
    }
  ]
}
```

#### Cline拡張との連携

```bash
# Cline拡張をインストール
# VS Code > Extensions > "Cline" で検索

# Clineの設定でOllamaを選択
# モデル: qwen3-coder:7b-instruct-q4_K_M
# APIエンドポイント: http://localhost:11434
```

### LM Studio セットアップ

1. https://lmstudio.ai/ からダウンロード
2. アプリ起動
3. モデル検索（例: "qwen2.5-coder"）
4. ダウンロードボタンをクリック
5. チャットで使用

**メリット**: GUIで直感的、初心者に優しい
**デメリット**: CLI自動化には不向き

## 10.6 量子化とパフォーマンス

### 量子化レベルの選択

| 量子化 | VRAM削減率 | 品質 | 推奨用途 |
|--------|----------|------|---------|
| **Q2_K** | 75% | ⭐⭐ | 非推奨（劣化大） |
| **Q3_K_M** | 62.5% | ⭐⭐⭐ | VRAM極小時のみ |
| **Q4_K_M** | 50% | ⭐⭐⭐⭐ | **標準推奨** |
| **Q5_K_M** | 37.5% | ⭐⭐⭐⭐⭐ | バランス良好 |
| **Q6_K** | 25% | ⭐⭐⭐⭐⭐ | 高品質 |
| **Q8_0** | 0% | ⭐⭐⭐⭐⭐ | ほぼFP16 |
| **FP16** | - | ⭐⭐⭐⭐⭐ | 最高品質（重い） |

### 実用的な選び方

**12GB VRAM**:
- 7B Q5_K_M（高品質）
- 14B Q4_K_M（大型モデル）

**16GB VRAM**:
- 7B Q6_K（最高品質）
- 14B Q5_K_M（バランス）

**24GB VRAM**:
- 32B Q4_K_M（大型モデル）
- 14B FP16（最高品質）

## 10.7 コスト比較：クラウド vs ローカル

### 月間1000リクエスト（平均500トークン入出力）の場合

| 選択肢 | 初期費用 | 月額コスト | 1年総額 | 3年総額 | 備考 |
|--------|---------|-----------|---------|---------|------|
| **Claude Sonnet 4.5 API** | ¥0 | ¥9,000 | ¥108,000 | ¥324,000 | 従量課金 |
| **Claude Pro/Team** | ¥0 | ¥2,800-4,500 | ¥33,600-54,000 | ¥100,800-162,000 | **CLI利用可**、制限あり |
| **ChatGPT Plus** | ¥0 | ¥2,800 | ¥33,600 | ¥100,800 | 制限あり |
| **ChatGPT Pro** | ¥0 | ¥28,000 | ¥336,000 | ¥1,008,000 | 無制限GPT-4o/o1 |
| **OpenAI API契約** | ¥0 | 変動 | - | - | **Codex CLI利用可** |
| **Gemini Advanced** | ¥0 | ¥2,800 | ¥33,600 | ¥100,800 | 高い上限 |
| **Microsoft 365 Copilot** | ¥0 | ¥4,200 | ¥50,400 | ¥151,200 | $30/月 |
| **ローカル（ミドル）** | ¥250,000 | ¥500* | ¥256,000 | ¥268,000 | 無制限 |
| **ローカル（ハイエンド）** | ¥500,000 | ¥800* | ¥509,600 | ¥528,800 | 無制限 |

*電気代のみ（GPU稼働時150W想定、¥30/kWh、月100時間使用）

**💡 重要: Max/Proプランの隠れたメリット**

サブスクリプションプランには、CLIツールの利用が含まれる場合があります：

- **Claude Pro/Team**: Claude CLI（claude-code）を使用可能。月額プラン内で一定量のCLI利用が可能。コーディング支援ツールとして追加費用なしで利用できるため、実質的なコストパフォーマンスが向上。

- **OpenAI API契約**: API契約者は、Codex CLIやOpenAI Code Interpreterへのアクセスが提供される場合があります（契約内容による）。大量のAPI利用者には優遇条件が適用されることも。

- **利用シーン**: WebUIでのチャット＋CLIでのコーディング支援の併用が可能になるため、ローカルLLMとクラウドサービスのハイブリッド運用がコスト効率良く実現できます。

### 月間5000リクエスト（ヘビーユーザー）の場合

| 選択肢 | 月額コスト | 1年総額 | 3年総額 | 備考 |
|--------|-----------|---------|---------|------|
| **Claude Sonnet 4.5 API** | ¥45,000 | ¥540,000 | ¥1,620,000 | 従量課金 |
| **ChatGPT Pro** | ¥28,000 | ¥336,000 | ¥1,008,000 | 無制限（おすすめ） |
| **ローカル（ミドル）** | ¥1,500* | ¥268,000 | ¥286,000 | **最コスパ** |
| **ローカル（ハイエンド）** | ¥2,400* | ¥528,800 | ¥557,600 | 高性能 |

*電気代（月300時間使用）

### 損益分岐点

- **ミドル構成**: 約28ヶ月でClaude Sonnet 4.5と同等
- **ハイエンド構成**: 約56ヶ月でClaude Sonnet 4.5と同等

### ローカルが有利なケース

✅ 大量使用（月3,000リクエスト以上）
✅ 長期利用予定（2年以上）
✅ プライバシー重視
✅ 複数人で共有
✅ オフライン必須

### クラウドが有利なケース

✅ 低頻度使用（月500リクエスト以下）
✅ 初期投資を避けたい
✅ 最新モデルを常に使いたい
✅ メンテナンス不要

## 10.8 推奨構成まとめ

### 2025年ベストプラクティス

#### 【最推奨】ミドルレンジ構成

```
GPU: RTX 4070 12GB - ¥80,000
CPU: Ryzen 7 7700X - ¥45,000
RAM: DDR5 64GB - ¥30,000
モデル: Qwen3-Coder 7B Q5_K_M

理由:
- コスパ最高
- 実用十分な性能（LiveCodeBench 47.2%）
- 電気代も控えめ
- 2年で元が取れる
```

#### 【プロ向け】ハイエンド構成

```
GPU: RTX 4090 24GB - ¥280,000
CPU: Ryzen 9 7950X3D - ¥80,000
RAM: DDR5 128GB - ¥60,000
モデル: Qwen3-Coder 32B Q4_K_M または DeepSeek V3.1

理由:
- Claude Sonnet 4レベルの性能
- 大規模プロジェクト対応（256K+コンテキスト）
- 将来性あり
```

#### 【コスパ重視】エントリー構成

```
GPU: RTX 4060 8GB - ¥40,000
CPU: Ryzen 5 7600 - ¥30,000
RAM: DDR5 32GB - ¥15,000
モデル: Qwen3-Coder 3B Q4_K_M または Flash 30B-A3B

理由:
- 最小投資
- Flashモデルで高性能（MoE）
- 既存PCのGPU追加でもOK
```

---

## 10.8.5 エージェンティックコーディングツール比較

ローカルLLMを使ってエージェンティックコーディングを行う場合、どのツールを使うかが重要です。

### 主要ツール比較表

| ツール | タイプ | ローカルLLM対応 | エージェント性 | 特徴 | おすすめ度 |
|--------|--------|----------------|--------------|------|-----------|
| **Cline** | VS Code拡張 | ⭐⭐⭐ (Ollama/LM Studio) | ⭐⭐⭐⭐⭐ | 最も自律的、ファイル操作可能 | 🥇 **最推奨** |
| **Continue** | VS Code拡張 | ⭐⭐⭐ (Ollama/LM Studio) | ⭐⭐⭐ | 補完特化、軽量 | 🥈 |
| **Cursor** | VSCodeフォーク | ⭐⭐ (複雑) | ⭐⭐⭐⭐ | UI最適化、独自サーバー | 🥉 |
| **Aider** | CLI | ⭐⭐⭐ (OpenAI互換API) | ⭐⭐⭐⭐ | Git連携最強、CLI派向け | 🥈 |
| **LiteLLM + Claude Code** | プロキシ経由 | ⭐⭐ (レイテンシ大) | ⭐⭐⭐⭐⭐ | 統一API、設定複雑 | ⚠️ 非推奨 |

### 詳細比較

#### 🥇 Cline（最推奨）

**旧名**: Claude Dev
**特徴**: 最もエージェンティック、自律的にファイル操作可能

**長所**:
- ✅ Ollama/LM Studio/Openrouterに直接接続（中間サーバーなし）
- ✅ ファイル作成・編集・削除を自動実行
- ✅ ターミナルコマンド実行可能
- ✅ セキュリティ: データはAPI直送（Azure/AWS/GCP対応）
- ✅ 日本語コーディング対応（qwen2.5-bakeneko-v2推奨）
- ✅ 完全無料、VS Code拡張

**設定例（Ollama）**:
```json
{
  "cline.apiProvider": "ollama",
  "cline.ollamaBaseUrl": "http://localhost:11434",
  "cline.ollamaModel": "qwen3-coder:7b-instruct-q4_K_M"
}
```

**推奨モデル**:
- **高速**: qwen3-coder:7b-instruct-q4_K_M
- **高品質**: qwen3-coder:32b-instruct-q4_K_M
- **日本語特化**: qwen2.5-bakeneko-v2:32b（rinnaモデル）

**使用感**: Claude Sonnet的な自律性、ただし応答品質はモデル依存

---

#### 🥈 Continue（補完特化）

**特徴**: コード補完とチャット両対応、軽量

**長所**:
- ✅ Tab補完（GitHub Copilot的）
- ✅ Ollama/LM Studio対応
- ✅ 軽量で高速
- ✅ カスタムプロンプト対応

**短所**:
- ❌ エージェント性はClineより低い
- ❌ ファイル操作の自律性が限定的

**推奨ケース**: リアルタイム補完メイン、チャットはサブ

---

#### 🥉 Cursor（VSCodeフォーク）

**特徴**: AI最適化されたエディタ全体

**長所**:
- ✅ UI/UX最適化（AI用に設計）
- ✅ エージェント性高い
- ✅ マルチファイル編集

**短所**:
- ❌ ローカルLLM設定が複雑（デフォルトは独自サーバー経由）
- ❌ データがCursorサーバーを経由（セキュリティ懸念）
- ❌ 有料プラン推奨（無料は制限あり）

**推奨ケース**: プロプライエタリモデル（GPT-4o/Claude）使用前提

---

#### 🥈 Aider（CLI型）

**特徴**: Git連携最強、CLI派向け

**長所**:
- ✅ Git統合（自動コミット、差分管理）
- ✅ OpenAI互換API対応（Ollama使用可）
- ✅ ベンチマーク高スコア（SWE-bench等）
- ✅ CLI統合で自動化しやすい

**設定例（Ollama）**:
```bash
export OPENAI_API_BASE=http://localhost:11434/v1
export OPENAI_API_KEY=ollama
aider --model qwen3-coder:7b-instruct-q4_K_M
```

**短所**:
- ❌ VS Code統合なし（エディタ別起動）
- ❌ GUI好きには不向き

**推奨ケース**: CLI派、Git操作自動化重視

---

#### ⚠️ LiteLLM + Claude Code（非推奨）

**特徴**: 100+ LLM APIを統一OpenAI形式で呼び出すプロキシ

**長所**:
- ✅ 複数LLMを統一API化
- ✅ ローカル/リモート混在可能

**短所**:
- ❌ **レイテンシが大きい**（プロキシ経由のオーバーヘッド）
- ❌ リアルタイム用途（チャット、音声）には不適
- ❌ エンタープライズ向け設定が複雑（手動スキャフォールディング必要）
- ❌ サービス検出、オートスケール、ログ統合などが非標準

**推奨ケース**: 複数LLM APIを切り替える必要がある場合のみ

**代替案**: Cline/Aiderで直接Ollama接続の方が高速・簡単

---

### 結論：おすすめツールランキング

#### 🥇 総合1位: **Cline**
- **理由**: ローカルLLM対応最強、エージェント性最高、無料、設定簡単
- **推奨環境**: VS Code + Ollama + Qwen3-Coder

#### 🥈 総合2位: **Aider**（CLI派）
- **理由**: Git連携最強、ベンチマーク高スコア
- **推奨環境**: ターミナル + Ollama + Qwen3-Coder

#### 🥉 総合3位: **Continue**（補完重視）
- **理由**: リアルタイム補完最強、軽量
- **推奨環境**: VS Code + Ollama + Qwen3-Coder（小型モデル）

#### ❌ 非推奨: LiteLLM経由
- **理由**: レイテンシ大、設定複雑、直接接続の方が高速

---

### 実践的セットアップ（Cline + Ollama）

**最速セットアップ**:

1. **Ollamaインストール**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

2. **モデルダウンロード**
```bash
ollama pull qwen3-coder:7b-instruct-q4_K_M
```

3. **VS CodeにCline拡張をインストール**
- 拡張機能から "Cline" を検索してインストール

4. **Cline設定**
- Clineアイコンクリック → Settings → Provider: "Ollama"
- Model: "qwen3-coder:7b-instruct-q4_K_M"

5. **コーディング開始**
- Clineチャットで「このReactコンポーネントをリファクタリングして」などと指示
- ファイル作成・編集を自動実行

**コスト**: 完全無料（電気代のみ）

---

## 10.9 セットアップガイド

### クイックスタート（Ollama + Qwen3-Coder）

**ステップ1**: Ollamaインストール
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**ステップ2**: モデルダウンロード
```bash
ollama pull qwen3-coder:7b-instruct-q4_K_M
```

**ステップ3**: VS Code拡張インストール
- "Continue" または "Cline" をインストール

**ステップ4**: 設定
```json
{
  "models": [{
    "title": "Qwen3-Coder",
    "provider": "ollama",
    "model": "qwen3-coder:7b-instruct-q4_K_M"
  }]
}
```

**ステップ5**: コーディング開始！
- Ctrl+L（Continue）または Clineアイコンクリック
- "この関数をリファクタリングして" などと指示

### トラブルシューティング

**問題**: "Out of Memory" エラー
**解決**: より小さいモデルまたはより強い量子化を使用
```bash
# 7B → 3Bに変更
ollama pull qwen3-coder:3b-instruct-q4_K_M

# またはFlashモデル（MoEで効率的）
ollama pull qwen3-coder:flash-q4_K_M
```

**問題**: 生成が遅い
**解決**: GPU使用を確認、量子化レベルを下げる
```bash
# nvidia-smiでGPU使用確認
nvidia-smi

# Q5 → Q4に変更で高速化
ollama pull qwen3-coder:7b-instruct-q4_K_M
```

**問題**: 品質が低い
**解決**: より大きいモデルまたはより弱い量子化
```bash
# 7B Q4 → 7B Q6に変更
ollama pull qwen3-coder:7b-instruct-q6_K

# または32Bモデルに変更
ollama pull qwen3-coder:32b-instruct-q4_K_M
```

## 10.10 まとめ：最適なローカルLLM選択フローチャート

```
コーディング用ローカルLLM選択
        ↓
   【VRAM容量は？】
        ↓
   ┌────┴────┐
   ↓         ↓
 8GB以下    12GB以上
   ↓         ↓
Qwen3      【用途は？】
Coder 3B      ↓
Q4_K_M   ┌────┴────┐
         ↓         ↓
      補完・生成  推論・デバッグ
         ↓         ↓
   Qwen3-Coder DeepSeek R1-0528
      7B          14B
    Q5_K_M      Q4_K_M

【24GB以上の場合】
    ↓
 Qwen3-Coder 32B Q4_K_M
 または DeepSeek V3.1
 （Claude Sonnet 4レベル）
```

### 最終推奨（2025年10月版）

🥇 **総合1位**: Qwen3-Coder 7B Q5_K_M
- 理由: 性能・VRAM・コストの最適バランス（LiveCodeBench 47.2%）

🥈 **推論特化**: DeepSeek R1-0528 14B Q4_K_M
- 理由: アルゴリズム設計、デバッグに最強（Codeforces ~1930）

🥉 **ハイブリッド**: DeepSeek V3.1
- 理由: 思考モードと高速性の両立（SWE-bench 66.0%）

🎖️ **軽量版**: Qwen3-Coder 3B Q4_K_M または Flash 30B-A3B
- 理由: 低スペックでもMoEで高性能

🎯 **エージェント型特化**: GLM-4-9B-Chat / GLM-4.6 🆕
- 理由: ツール呼び出し90.6%（業界最高）、GLM-4.6はAIME 98.6%で数理推論最強

🌙 **超長文対応**: Kimi K2
- 理由: 256Kコンテキスト、レポジトリ全体分析に最適

🖼️ **ビジュアルコーディング**: Qwen3-VL-8B-Instruct
- 理由: 画像→コード変換、OCRBench 896点、UI/UX開発最強

🔓 **OpenAI公式**: OpenAI GPT-OSS 🆕
- 理由: GPT-2以来6年ぶりのオープンウェイト、MIT License、商用利用完全自由

---

## 関連リンク

### 実行環境
- [Ollama公式](https://ollama.com/)
- [LM Studio](https://lmstudio.ai/)

### モデル
- [Qwen3-Coder - Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-7B-Instruct)
- [Qwen3-Coder - GitHub](https://github.com/QwenLM/Qwen3-Coder)
- [Qwen3-VL - Hugging Face](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)
- [Qwen3-VL - GitHub](https://github.com/QwenLM/Qwen3-VL)
- [DeepSeek R1-0528](https://github.com/deepseek-ai/DeepSeek-R1)
- [DeepSeek V3.1](https://github.com/deepseek-ai/DeepSeek-V3)
- [OpenAI GPT-OSS - GitHub](https://github.com/openai/gpt-oss) 🆕
- [OpenAI GPT-OSS - Hugging Face](https://huggingface.co/openai/gpt-oss-20b) 🆕
- [GLM-4 - GitHub](https://github.com/THUDM/GLM-4)
- [GLM-4-9B-Chat - Hugging Face](https://huggingface.co/THUDM/glm-4-9b-chat-hf)
- [Kimi K2 - GitHub](https://github.com/MoonshotAI/Kimi-K2)
- [Kimi K2 - Hugging Face](https://huggingface.co/moonshotai/Kimi-K2-Instruct)
- [Phi-4](https://huggingface.co/microsoft/phi-4)
- [Gemma 3](https://huggingface.co/google/gemma-3-12b)

### VS Code拡張
- [Continue VS Code拡張](https://continue.dev/)
- [Cline VS Code拡張](https://github.com/cline/cline)
