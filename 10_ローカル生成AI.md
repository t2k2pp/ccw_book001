# 第10章: ローカル生成AI - コーディング特化モデルカタログ

プライバシー、コスト、オフライン利用を重視する開発者にとって、ローカル環境で動作するLLMは重要な選択肢です。本章では、コーディング用途に特化したローカルLLMを、マシンスペックと目的に応じて選択できるよう体系的にカタログ化します。

## 10.1 ローカルLLMの概要とメリット

### なぜローカルLLMなのか？

```
クラウドAI                vs              ローカルAI
  ↓                                         ↓
● 高性能                              ● プライバシー保護
● 常に最新                            ● コスト削減（従量課金なし）
● セットアップ不要                    ● オフライン利用可能
● 月額/従量課金                       ● カスタマイズ自由
                                      ● 企業コード流出リスクゼロ
```

### ローカルLLMが最適なケース

✅ **プライバシー重視**
- 企業の機密コードを外部に送信したくない
- セキュリティポリシーでクラウドAI利用が制限される

✅ **コスト削減**
- 大量のコード生成で従量課金が高額になる
- 初期投資後はランニングコストなし

✅ **オフライン環境**
- インターネット接続が不安定
- 機内、移動中でも作業したい

✅ **カスタマイズ**
- 特定のコーディングスタイルに特化させたい
- 社内フレームワーク用にファインチューニング

### 2025年のローカルLLMの進化

2025年のローカルLLMコーディング環境は大きく成熟しました：

- **性能向上**: Qwen 2.5 CoderはClaude Sonnet 4に匹敵
- **小型化**: 1.5B〜7Bモデルでも実用的な性能
- **ツールの充実**: Ollama, LM Studioで簡単セットアップ
- **量子化技術**: 4bit/8bit量子化でVRAM消費を大幅削減

## 10.2 マシンスペック別推奨モデル

### スペック分類とVRAM要件

| スペック | VRAM | 推奨モデルサイズ | 快適度 | 想定ハードウェア |
|---------|------|----------------|--------|----------------|
| **エントリー** | 4-8GB | 1.5B-3B (Q4) | ⭐⭐ | GTX 1660, RTX 3050 |
| **ミドル** | 12-16GB | 7B-8B (Q5-Q6) | ⭐⭐⭐⭐ | RTX 3060, RTX 4060 Ti |
| **ハイエンド** | 24GB | 14B-32B (Q4-Q5) | ⭐⭐⭐⭐⭐ | RTX 3090, RTX 4090 |
| **プロ/ワークステーション** | 40GB+ | 70B+ (Q4) | ⭐⭐⭐⭐⭐ | RTX 4090×2, A100 |

### メモリ計算式

**基本式**:
```
必要VRAM (GB) ≈ モデルパラメータ数 (B) × 量子化ビット数 / 8 × 1.2

例: 7B Q4モデル
= 7 × 4 / 8 × 1.2 = 4.2GB
```

**実用的な目安**:
- 7B Q4: 約4-5GB
- 14B Q4: 約8-9GB
- 32B Q4: 約18-20GB
- 70B Q4: 約42-48GB

### スペック別推奨構成（2025年版）

#### エントリー構成（予算15万円）
```
GPU: RTX 4060 (8GB) - ¥40,000
CPU: Ryzen 5 7600 - ¥30,000
RAM: DDR5 32GB - ¥15,000
SSD: 1TB NVMe - ¥10,000
その他: ¥55,000

推奨モデル: Qwen 2.5 Coder 3B, Phi-3 Mini
```

#### ミドル構成（予算25万円）
```
GPU: RTX 4070 (12GB) - ¥80,000
CPU: Ryzen 7 7700X - ¥45,000
RAM: DDR5 64GB - ¥30,000
SSD: 2TB NVMe - ¥15,000
その他: ¥80,000

推奨モデル: Qwen 2.5 Coder 7B, DeepSeek Coder 6.7B
```

#### ハイエンド構成（予算50万円）
```
GPU: RTX 4090 (24GB) - ¥280,000
CPU: Ryzen 9 7950X - ¥80,000
RAM: DDR5 128GB - ¥60,000
SSD: 4TB NVMe - ¥30,000
その他: ¥50,000

推奨モデル: Qwen 2.5 Coder 32B, DeepSeek R1 14B, Llama 3.3 70B (Q4)
```

#### プロ構成（予算100万円+）
```
GPU: RTX 4090 × 2 (48GB total) - ¥560,000
または RTX 5080 32GB × 2 (64GB total)
CPU: Ryzen Threadripper - ¥150,000
RAM: DDR5 256GB - ¥120,000
SSD: 8TB NVMe - ¥60,000
その他: ¥110,000

推奨モデル: Qwen 2.5 Coder 72B, DeepSeek V3, Llama 3.3 70B (Q6)
```

#### 🆕 コンパクトワークステーション構成（AMD Strix Halo）

**MINISFORUM MS-S1 MAX** ¥345,000 (約$2,299)

```
APU: AMD Ryzen AI Max+ 395 (16コアZen 5)
統合GPU: Radeon 8060S (40 RDNA 3.5 CU、最大96GB VRAM利用可)
NPU: XDNA 2 (50 TOPS) + 総合126 TOPS AI性能
RAM: 128GB LPDDR5X-8000 (256GB/s帯域幅)
TDP: 110-160W（4段階調整可能）
電源: 320W内蔵PSU
拡張: PCIe 4.0 x16スロット（x4配線）、デュアルUSB4 V2 (80Gbps)
サイズ: ミニPC（超コンパクト）

特徴:
✅ 70B〜100Bパラメータモデルをローカル実行可能
✅ 統合GPUで96GBまでVRAM利用（業界最大級）
✅ 2台クラスタで235B Q4モデルが10.87 tok/sec
✅ デスクトップ級GPU性能（RTX 4060相当）
✅ 省スペース・低消費電力（160W MAX）

推奨モデル:
- Qwen 2.5 Coder 72B Q4
- Llama 3.3 70B Q4-Q5
- DeepSeek Coder V3 33B Q6
- Gemma 3 27B Q6

評価: ⭐⭐⭐⭐⭐ - コンパクトで超大型LLM実行可能
```

**Strix Haloが最適なケース**:
- 省スペースで高性能が必要
- 70B以上のモデルを動かしたいが、デュアルGPU構成は避けたい
- 電力効率を重視
- 将来的にクラスタ構成を検討

**注意点**:
- 専用GPUと比較してやや低速（RTX 4090比で30-40%低速）
- メモリ帯域がボトルネックになる場合あり
- 日本での入手性は要確認

---

## 10.3 コーディング特化モデルカタログ（2025年版）

### 総合性能ランキング（2025年10月時点）

| 順位 | モデル | 開発元 | パラメータ | HumanEval | LiveCodeBench | 総合評価 |
|-----|--------|--------|-----------|------------|--------------|---------|
| 🥇 1 | **Qwen 2.5 Coder** | Alibaba | 1.5B-72B | 88.3% (7B) | 38.7% | ⭐⭐⭐⭐⭐ |
| 🥈 2 | **DeepSeek R1** | DeepSeek | 1.5B-70B | - | 74.8% (14B) | ⭐⭐⭐⭐⭐ |
| 🥉 3 | **Phi-4** | Microsoft | 3.8B-14B | 82.6 (14B) | 53.8% (14B) | ⭐⭐⭐⭐⭐ |
| 4 | **DeepSeek Coder V3** | DeepSeek | 6.7B-236B | - | 37.6% | ⭐⭐⭐⭐⭐ |
| 5 | **Gemma 3** | Google | 1B-27B | ⭐高 | ⭐高 | ⭐⭐⭐⭐⭐ |
| 6 | **Llama 3.3** | Meta | 70B | - | - | ⭐⭐⭐⭐ |
| 7 | **CodeLlama 3** | Meta | 7B-70B | - | - | ⭐⭐⭐⭐ |
| 8 | **StarCoder2** | BigCode | 3B-15B | - | - | ⭐⭐⭐⭐ |
| 9 | **Gemma 3n** | Google | E2B/E4B | - | - | ⭐⭐⭐⭐ |

### 詳細モデルカタログ

---

#### 🏆 Qwen 2.5 Coder（最推奨）

**開発元**: Alibaba Qwen Team
**リリース**: 2025年7月
**ライセンス**: Apache 2.0

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 適したスペック | 用途 |
|--------|----------|--------------|------|
| 1.5B | 1.5GB | エントリー | 軽量補完 |
| 3B | 2.5GB | エントリー | 基本コード生成 |
| 7B | 4.5GB | ミドル | 汎用コーディング |
| 14B | 9GB | ハイエンド | 高品質生成 |
| 32B | 20GB | ハイエンド | 複雑リファクタリング |
| 72B | 45GB | プロ | Claude Sonnet 4レベル |

**強み**:
- 2025年最高のコーディング性能（Aider-Edit 84.2%）
- 大規模コンテキスト対応（128K tokens）
- 多言語コード生成に優れる
- エージェント型コーディングに最適

**ベンチマーク**:
- HumanEval: 88.3% (7B)
- MBPP: 85.7% (7B)
- Claude Sonnet 4に匹敵（32B以上）

**推奨量子化**:
- 12GB VRAM: 7B-Q5
- 16GB VRAM: 7B-Q6 または 14B-Q4
- 24GB VRAM: 32B-Q4

**Ollamaでの使用**:
```bash
# インストール
ollama pull qwen2.5-coder:7b-instruct-q4_K_M

# 実行
ollama run qwen2.5-coder:7b-instruct-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - 2025年の最強コーディングLLM

---

#### 🔥 DeepSeek R1（推論特化）

**開発元**: DeepSeek
**リリース**: 2025年1月
**ライセンス**: MIT

**特徴**:
- 推論プロセスを可視化
- 数学・論理的思考に強い
- 日本語版あり（Lightblue/サイバーエージェント）

**モデルバリエーション**:
| サイズ | VRAM (Q4) | LiveCodeBench | 特徴 |
|--------|----------|--------------|------|
| 1.5B | 1.5GB | - | 軽量版 |
| 7B | 4.5GB | - | 標準版 |
| 14B | 9GB | 74.8% | ベストバランス |
| 70B | 45GB | - | 最高性能 |

**強み**:
- 複雑なアルゴリズム設計
- デバッグ支援（思考過程を表示）
- アーキテクチャ設計の提案

**Ollamaでの使用**:
```bash
# DeepSeek R1 14B (推奨)
ollama pull deepseek-r1:14b-qwen-distill-q4_K_M

# 日本語特化版
ollama pull lightblue/deepseek-r1-distill-qwen-14b-japanese
```

**評価**: ⭐⭐⭐⭐⭐ - 推論タスク最強

---

#### 🚀 DeepSeek Coder V3

**開発元**: DeepSeek
**リリース**: 2024年

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 特徴 |
|--------|----------|------|
| 6.7B | 4.2GB | コンパクト高性能 |
| 33B | 20GB | ハイエンド |

**強み**:
- Fill-in-the-Middle対応（カーソル位置でのコード補完）
- 多言語対応（80+プログラミング言語）
- レポジトリレベルのコード理解

**評価**: ⭐⭐⭐⭐⭐

---

#### 🦙 CodeLlama 3

**開発元**: Meta
**リリース**: 2024年
**ライセンス**: Llama 3 License

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 用途 |
|--------|----------|------|
| 7B | 4.5GB | 基本 |
| 13B | 8GB | 中級 |
| 34B | 21GB | 高度 |
| 70B | 45GB | 最高 |

**強み**:
- 安定性が高い
- Fill-in-the-Middle対応
- Python特化版あり

**注意**: Qwen 2.5 Coderに性能で劣るため、新規導入は推奨しない。既存ユーザーは移行を検討。

**評価**: ⭐⭐⭐⭐ - 安定だが世代交代が進行中

---

#### 🔵 Phi-4（最新 - コーディング特化）

**開発元**: Microsoft
**リリース**: 2025年1月
**ライセンス**: MIT

**モデルバリエーション**:
| サイズ | VRAM (Q4) | 特徴 |
|--------|----------|------|
| Phi-4-Mini (3.8B) | 2.5GB | 軽量版 |
| Phi-4 (14B) | 9GB | 標準版 |
| Phi-4-reasoning (14B) | 9GB | 推論特化 |

**強み**:
- **HumanEval 82.6**: LLaMA 3.3 70Bを上回る驚異的なスコア
- **LiveCodeBench 53.8%**: コーディング性能が大幅向上（+25ポイント）
- **MATH 80.4**: 数学的推論も優秀
- 14Bで70Bクラスの性能を実現

**ベンチマーク**:
- HumanEval: 82.6
- LiveCodeBench: 53.8%
- MATH: 80.4
- GSM-8K: 88.6% (Mini版)

**推奨用途**:
- 中スペックGPU（12-16GB）でのハイパフォーマンス
- 数学・論理的コーディングタスク
- コンパクトながら高品質な生成が必要な場合

**Ollamaでの使用**:
```bash
# Phi-4 14B (推奨)
ollama pull phi4:14b-q4_K_M

# Phi-4 Mini 3.8B (軽量版)
ollama pull phi4:mini-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - 14Bで最高クラスのコーディング性能

---

#### ⚡ Phi-3 Mini（旧世代）

**開発元**: Microsoft
**サイズ**: 3.8B
**VRAM**: 2.5GB (Q4)

**強み**:
- 非常に軽量
- GPUなしでもCPUで動作可能
- モバイルデバイス対応

**用途**:
- 低スペックPC
- ラップトップでの軽量補完
- 学習用

**注意**: Phi-4 Miniの登場により、新規導入は推奨しない。

**評価**: ⭐⭐⭐ - 軽量だが性能は控えめ（Phi-4への移行推奨）

---

#### 🔷 Gemma 3（Google最新・マルチモーダル）

**開発元**: Google / DeepMind
**リリース**: 2025年3月
**ライセンス**: Gemma License（商用可）
**ベース技術**: Gemini 2.0と同じ研究・技術

**モデルバリエーション**:
| サイズ | VRAM (Q4) | マルチモーダル | コンテキスト | 特徴 |
|--------|----------|--------------|-------------|------|
| 270M | 0.5GB | テキストのみ | - | 超軽量ファインチューニング用 |
| 1B | 1GB | テキストのみ | 128K | モバイル向け |
| 4B | 3GB | 画像+テキスト | 128K | バランス型 |
| 12B | 8GB | 画像+テキスト | 128K | 高性能 |
| 27B | 17GB | 画像+テキスト | 128K | 最高性能 |

**強み**:
- **マルチモーダル**: 画像とテキストを同時処理（4B以上）
- **大規模コンテキスト**: 128K tokens（約300ページの本、30枚の高解像度画像、1時間以上の動画）
- **Gemini 2.0ベース**: Googleの最先端技術を凝縮
- コーディング、数学、推論で優れた性能
- 140+言語対応

**ベンチマーク**:
- HumanEval・MBPPで高スコア
- 27BはA100 80GB・H100で効率的に動作
- 世界最高のシングルアクセラレータモデル（27B）

**推奨用途**:
- マルチモーダルコーディング（コード + 図解）
- 多言語コード生成
- 数学・科学系コーディング
- モバイル/エッジデバイス（1B、270M）

**Ollamaでの使用**:
```bash
# Gemma 3 12B (推奨バランス型)
ollama pull gemma3:12b-instruct-q4_K_M

# Gemma 3 27B (高性能)
ollama pull gemma3:27b-instruct-q4_K_M
```

**評価**: ⭐⭐⭐⭐⭐ - Gemini 2.0技術のオープン版

---

#### 📱 Gemma 3n（モバイル特化・効率重視）

**開発元**: Google / DeepMind
**リリース**: 2025年6月
**ライセンス**: Gemma License（商用可）
**特徴**: Gemini Nanoと同じアーキテクチャ

**モデルバリエーション**:
| モデル | 実際のパラメータ | 実効メモリ | マルチモーダル |
|--------|----------------|-----------|--------------|
| E2B | 5B | 2GB | 音声+画像+テキスト+動画 |
| E4B | 8B | 3GB | 音声+画像+テキスト+動画 |

**革新的アーキテクチャ**:
- **Matryoshka Transformer (MatFormer)**: 大きなモデル内に小さなモデルをネスト
- **Per-Layer Embedding (PLE)**: レイヤーごとの埋め込みでパフォーマンス向上
- **MobileNet-V5-300M**: 新世代ビジョンエンコーダー
- **音声処理**: 30秒までの音声クリップを処理（転写・翻訳）

**強み**:
- 5Bパラメータが2GBメモリで動作（従来の2Bモデル相当）
- モバイル・エッジデバイスで最高クラスの性能
- 音声、画像、動画、テキストを統合処理

**推奨用途**:
- モバイルアプリ開発
- エッジAIコーディング支援
- 低メモリ環境での高性能推論
- マルチモーダルタスク

**評価**: ⭐⭐⭐⭐ - モバイル/エッジ最強モデル

---

#### 🌟 StarCoder2

**開発元**: BigCode
**サイズ**: 3B, 7B, 15B

**強み**:
- オープンソースコードで学習
- 透明性が高い
- コミュニティ主導

**評価**: ⭐⭐⭐⭐

---

#### 🦙 Llama 3.3

**開発元**: Meta
**サイズ**: 70B
**VRAM**: 45GB (Q4)

**特徴**:
- 汎用LLMとしての性能
- コーディング以外も強い
- 推論能力向上

**評価**: ⭐⭐⭐⭐ - 汎用性は高いがコーディング特化には劣る

---

## 10.4 用途別モデル選択ガイド

### シーン1: 日常的なコード補完

**目的**: VS Codeでリアルタイム補完

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| エントリー | Qwen 2.5 Coder 3B Q4 | 軽量で高速 |
| ミドル | Qwen 2.5 Coder 7B Q5 | ベストバランス |
| ハイエンド | Qwen 2.5 Coder 14B Q4 | 最高品質 |

---

### シーン2: 複雑なアルゴリズム設計

**目的**: データ構造やアルゴリズムの実装

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | DeepSeek R1 7B | 推論過程が見える |
| ハイエンド | DeepSeek R1 14B | 最適 |
| プロ | Qwen 2.5 Coder 32B | 最高性能 |

---

### シーン3: リファクタリング

**目的**: 既存コードの改善

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | Qwen 2.5 Coder 7B Q6 | コードベース理解 |
| ハイエンド | Qwen 2.5 Coder 32B Q4 | 大規模リファクタリング |

---

### シーン4: デバッグ支援

**目的**: バグの原因特定と修正

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ミドル | DeepSeek R1 7B | 思考過程でバグ特定 |
| ハイエンド | DeepSeek R1 14B | 複雑なバグ対応 |

---

### シーン5: プロジェクト全体の理解

**目的**: 大規模コードベースの分析

| スペック | 推奨モデル | 理由 |
|---------|-----------|------|
| ハイエンド | Qwen 2.5 Coder 32B | 128Kコンテキスト |
| プロ | Qwen 2.5 Coder 72B | 最大理解力 |

---

## 10.5 ローカルLLM実行ツール

### Ollama vs LM Studio 比較

| 特徴 | Ollama | LM Studio |
|------|--------|-----------|
| **インターフェース** | CLI + 簡易GUI | フルGUI |
| **使いやすさ** | 開発者向け | 初心者向け |
| **柔軟性** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| **API提供** | REST API標準 | あり |
| **対応OS** | Mac, Linux, Windows | Mac, Windows |
| **モデル管理** | CLI | GUI（ドラッグ&ドロップ） |
| **推奨用途** | 開発統合、自動化 | 手軽に試す、チャット |

### Ollama セットアップ（推奨）

#### インストール

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# https://ollama.com/download からインストーラーをダウンロード
```

#### モデルのダウンロードと実行

```bash
# Qwen 2.5 Coder 7B (推奨)
ollama pull qwen2.5-coder:7b-instruct-q4_K_M
ollama run qwen2.5-coder:7b-instruct-q4_K_M

# DeepSeek R1 14B
ollama pull deepseek-r1:14b-qwen-distill-q4_K_M
ollama run deepseek-r1:14b-qwen-distill-q4_K_M

# CodeLlama 7B
ollama pull codellama:7b-instruct
ollama run codellama:7b-instruct
```

#### VS Code統合（Continue拡張）

```bash
# Continue拡張をインストール
# VS Code > Extensions > "Continue" で検索

# 設定例 (~/.continue/config.json)
{
  "models": [
    {
      "title": "Qwen 2.5 Coder",
      "provider": "ollama",
      "model": "qwen2.5-coder:7b-instruct-q4_K_M"
    }
  ]
}
```

#### Cline拡張との連携

```bash
# Cline拡張をインストール
# VS Code > Extensions > "Cline" で検索

# Clineの設定でOllamaを選択
# モデル: qwen2.5-coder:7b-instruct-q4_K_M
# APIエンドポイント: http://localhost:11434
```

### LM Studio セットアップ

1. https://lmstudio.ai/ からダウンロード
2. アプリ起動
3. モデル検索（例: "qwen2.5-coder"）
4. ダウンロードボタンをクリック
5. チャットで使用

**メリット**: GUIで直感的、初心者に優しい
**デメリット**: CLI自動化には不向き

## 10.6 量子化とパフォーマンス

### 量子化レベルの選択

| 量子化 | VRAM削減率 | 品質 | 推奨用途 |
|--------|----------|------|---------|
| **Q2_K** | 75% | ⭐⭐ | 非推奨（劣化大） |
| **Q3_K_M** | 62.5% | ⭐⭐⭐ | VRAM極小時のみ |
| **Q4_K_M** | 50% | ⭐⭐⭐⭐ | **標準推奨** |
| **Q5_K_M** | 37.5% | ⭐⭐⭐⭐⭐ | バランス良好 |
| **Q6_K** | 25% | ⭐⭐⭐⭐⭐ | 高品質 |
| **Q8_0** | 0% | ⭐⭐⭐⭐⭐ | ほぼFP16 |
| **FP16** | - | ⭐⭐⭐⭐⭐ | 最高品質（重い） |

### 実用的な選び方

**12GB VRAM**:
- 7B Q5_K_M（高品質）
- 14B Q4_K_M（大型モデル）

**16GB VRAM**:
- 7B Q6_K（最高品質）
- 14B Q5_K_M（バランス）

**24GB VRAM**:
- 32B Q4_K_M（大型モデル）
- 14B FP16（最高品質）

## 10.7 コスト比較：クラウド vs ローカル

### 月間1000リクエスト（平均500トークン入出力）の場合

| 選択肢 | 初期費用 | 月額コスト | 1年総額 | 3年総額 | 備考 |
|--------|---------|-----------|---------|---------|------|
| **Claude Sonnet 4.5 API** | ¥0 | ¥9,000 | ¥108,000 | ¥324,000 | 従量課金 |
| **Claude Pro/Team** | ¥0 | ¥2,800-4,500 | ¥33,600-54,000 | ¥100,800-162,000 | **CLI利用可**、制限あり |
| **ChatGPT Plus** | ¥0 | ¥2,800 | ¥33,600 | ¥100,800 | 制限あり |
| **ChatGPT Pro** | ¥0 | ¥28,000 | ¥336,000 | ¥1,008,000 | 無制限GPT-4o/o1 |
| **OpenAI API契約** | ¥0 | 変動 | - | - | **Codex CLI利用可** |
| **Gemini Advanced** | ¥0 | ¥2,800 | ¥33,600 | ¥100,800 | 高い上限 |
| **Microsoft 365 Copilot** | ¥0 | ¥4,200 | ¥50,400 | ¥151,200 | $30/月 |
| **ローカル（ミドル）** | ¥250,000 | ¥500* | ¥256,000 | ¥268,000 | 無制限 |
| **ローカル（ハイエンド）** | ¥500,000 | ¥800* | ¥509,600 | ¥528,800 | 無制限 |

*電気代のみ（GPU稼働時150W想定、¥30/kWh、月100時間使用）

**💡 重要: Max/Proプランの隠れたメリット**

サブスクリプションプランには、CLIツールの利用が含まれる場合があります：

- **Claude Pro/Team**: Claude CLI（claude-code）を使用可能。月額プラン内で一定量のCLI利用が可能。コーディング支援ツールとして追加費用なしで利用できるため、実質的なコストパフォーマンスが向上。

- **OpenAI API契約**: API契約者は、Codex CLIやOpenAI Code Interpreterへのアクセスが提供される場合があります（契約内容による）。大量のAPI利用者には優遇条件が適用されることも。

- **利用シーン**: WebUIでのチャット＋CLIでのコーディング支援の併用が可能になるため、ローカルLLMとクラウドサービスのハイブリッド運用がコスト効率良く実現できます。

### 月間5000リクエスト（ヘビーユーザー）の場合

| 選択肢 | 月額コスト | 1年総額 | 3年総額 | 備考 |
|--------|-----------|---------|---------|------|
| **Claude Sonnet 4.5 API** | ¥45,000 | ¥540,000 | ¥1,620,000 | 従量課金 |
| **ChatGPT Pro** | ¥28,000 | ¥336,000 | ¥1,008,000 | 無制限（おすすめ） |
| **ローカル（ミドル）** | ¥1,500* | ¥268,000 | ¥286,000 | **最コスパ** |
| **ローカル（ハイエンド）** | ¥2,400* | ¥528,800 | ¥557,600 | 高性能 |

*電気代（月300時間使用）

### 損益分岐点

- **ミドル構成**: 約28ヶ月でClaude Sonnet 4.5と同等
- **ハイエンド構成**: 約56ヶ月でClaude Sonnet 4.5と同等

### ローカルが有利なケース

✅ 大量使用（月3,000リクエスト以上）
✅ 長期利用予定（2年以上）
✅ プライバシー重視
✅ 複数人で共有
✅ オフライン必須

### クラウドが有利なケース

✅ 低頻度使用（月500リクエスト以下）
✅ 初期投資を避けたい
✅ 最新モデルを常に使いたい
✅ メンテナンス不要

## 10.8 推奨構成まとめ

### 2025年ベストプラクティス

#### 【最推奨】ミドルレンジ構成

```
GPU: RTX 4070 12GB - ¥80,000
CPU: Ryzen 7 7700X - ¥45,000
RAM: DDR5 64GB - ¥30,000
モデル: Qwen 2.5 Coder 7B Q5_K_M

理由:
- コスパ最高
- 実用十分な性能（Aider-Edit 84%+）
- 電気代も控えめ
- 2年で元が取れる
```

#### 【プロ向け】ハイエンド構成

```
GPU: RTX 4090 24GB - ¥280,000
CPU: Ryzen 9 7950X3D - ¥80,000
RAM: DDR5 128GB - ¥60,000
モデル: Qwen 2.5 Coder 32B Q4_K_M

理由:
- Claude Sonnet 4レベルの性能
- 大規模プロジェクト対応
- 将来性あり
```

#### 【コスパ重視】エントリー構成

```
GPU: RTX 4060 8GB - ¥40,000
CPU: Ryzen 5 7600 - ¥30,000
RAM: DDR5 32GB - ¥15,000
モデル: Qwen 2.5 Coder 3B Q4_K_M

理由:
- 最小投資
- 軽量補完には十分
- 既存PCのGPU追加でもOK
```

## 10.9 セットアップガイド

### クイックスタート（Ollama + Qwen 2.5 Coder）

**ステップ1**: Ollamaインストール
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**ステップ2**: モデルダウンロード
```bash
ollama pull qwen2.5-coder:7b-instruct-q4_K_M
```

**ステップ3**: VS Code拡張インストール
- "Continue" または "Cline" をインストール

**ステップ4**: 設定
```json
{
  "models": [{
    "title": "Qwen 2.5 Coder",
    "provider": "ollama",
    "model": "qwen2.5-coder:7b-instruct-q4_K_M"
  }]
}
```

**ステップ5**: コーディング開始！
- Ctrl+L（Continue）または Clineアイコンクリック
- "この関数をリファクタリングして" などと指示

### トラブルシューティング

**問題**: "Out of Memory" エラー
**解決**: より小さいモデルまたはより強い量子化を使用
```bash
# 7B → 3Bに変更
ollama pull qwen2.5-coder:3b-instruct-q4_K_M
```

**問題**: 生成が遅い
**解決**: GPU使用を確認、量子化レベルを下げる
```bash
# nvidia-smiでGPU使用確認
nvidia-smi

# Q5 → Q4に変更で高速化
ollama pull qwen2.5-coder:7b-instruct-q4_K_M
```

**問題**: 品質が低い
**解決**: より大きいモデルまたはより弱い量子化
```bash
# 7B Q4 → 7B Q6に変更
ollama pull qwen2.5-coder:7b-instruct-q6_K
```

## 10.10 まとめ：最適なローカルLLM選択フローチャート

```
コーディング用ローカルLLM選択
        ↓
   【VRAM容量は？】
        ↓
   ┌────┴────┐
   ↓         ↓
 8GB以下    12GB以上
   ↓         ↓
Qwen 2.5   【用途は？】
Coder 3B      ↓
Q4_K_M   ┌────┴────┐
         ↓         ↓
      補完・生成  推論・デバッグ
         ↓         ↓
    Qwen 2.5   DeepSeek R1
    Coder 7B      14B
    Q5_K_M     Q4_K_M

【24GB以上の場合】
    ↓
 Qwen 2.5 Coder 32B Q4_K_M
 （Claude Sonnet 4レベル）
```

### 最終推奨（2025年10月版）

🥇 **総合1位**: Qwen 2.5 Coder 7B Q5_K_M
- 理由: 性能・VRAM・コストの最適バランス

🥈 **推論特化**: DeepSeek R1 14B Q4_K_M
- 理由: アルゴリズム設計、デバッグに最強

🥉 **軽量版**: Qwen 2.5 Coder 3B Q4_K_M
- 理由: 低スペックでも実用的

---

## 関連リンク
- [Ollama公式](https://ollama.com/)
- [LM Studio](https://lmstudio.ai/)
- [Qwen 2.5 Coder - Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)
- [DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1)
- [Continue VS Code拡張](https://continue.dev/)
- [Cline VS Code拡張](https://github.com/cline/cline)
